@inproceedings{ashok2025language,
title={Language Models Can Predict Their Own Behavior},
author={Dhananjay Ashok and Jonathan May},
booktitle={The Thirty-ninth Annual Conference on Neural Information Processing Systems},
year={2025},
url={https://openreview.net/forum?id=i8IqEzpHaJ}
}

@inproceedings{ashok-etal-2025-vlms,
    title = "Can {VLM}s Recall Factual Associations From Visual References?",
    author = "Ashok, Dhananjay  and
      Chaubey, Ashutosh  and
      Arai, Hirona Jacqueline  and
      May, Jonathan  and
      Thomason, Jesse",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2025",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-emnlp.850/",
    doi = "10.18653/v1/2025.findings-emnlp.850",
    pages = "15691--15708",
    ISBN = "979-8-89176-335-7"
}

@inproceedings{huang-etal-2025-teaching,
    title = "Teaching Language Models To Gather Information Proactively",
    author = "Huang, Tenghao  and
      Chen, Sihao  and
      Chen, Muhao  and
      May, Jonathan  and
      Yang, Longqi  and
      Wan, Mengting  and
      Zhou, Pei",
    editor = "Christodoulopoulos, Christos  and
      Chakraborty, Tanmoy  and
      Rose, Carolyn  and
      Peng, Violet",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2025",
    month = nov,
    year = "2025",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-emnlp.843/",
    doi = "10.18653/v1/2025.findings-emnlp.843",
    pages = "15588--15599",
    ISBN = "979-8-89176-335-7"
}


@inproceedings{huang-etal-2025-r2d2,
    title = "{R}2{D}2: Remembering, Replaying and Dynamic Decision Making with a Reflective Agentic Memory",
    author = "Huang, Tenghao  and
      Basu, Kinjal  and
      Abdelaziz, Ibrahim  and
      Kapanipathi, Pavan  and
      May, Jonathan  and
      Chen, Muhao",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1464/",
    doi = "10.18653/v1/2025.acl-long.1464",
    pages = "30318--30330",
    ISBN = "979-8-89176-251-0"
}


@inproceedings{spangher-etal-2025-newsinterview,
    title = "{N}ews{I}nterview: a Dataset and a Playground to Evaluate {LLM}s' Grounding Gap via Informational Interviews",
    author = "Spangher, Alexander  and
      Lu, Michael  and
      Kalyan, Sriya  and
      Cho, Hyundong Justin  and
      Huang, Tenghao  and
      Shi, Weiyan  and
      May, Jonathan",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.1580/",
    doi = "10.18653/v1/2025.acl-long.1580",
    pages = "32895--32925",
    ISBN = "979-8-89176-251-0"
}

@inproceedings{ashok-may-2025-little,
    title = "A Little Human Data Goes A Long Way",
    author = "Ashok, Dhananjay  and
      May, Jonathan",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-short.30/",
    doi = "10.18653/v1/2025.acl-short.30",
    pages = "381--413",
    ISBN = "979-8-89176-252-7"
}


@inproceedings{guo-etal-2025-learning,
    title = "Learning to Rewrite Negation Queries in Product Search",
    author = "Guo, Mengtian  and
      Al-Darabsah, Mutasem  and
      Teo, Choon Hui  and
      May, Jonathan  and
      Agarwal, Tarun  and
      Bhagat, Rahul",
    editor = "Rambow, Owen  and
      Wanner, Leo  and
      Apidianaki, Marianna  and
      Al-Khalifa, Hend  and
      Eugenio, Barbara Di  and
      Schockaert, Steven  and
      Darwish, Kareem  and
      Agarwal, Apoorv",
    booktitle = "Proceedings of the 31st International Conference on Computational Linguistics: Industry Track",
    month = jan,
    year = "2025",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.coling-industry.49/",
    pages = "575--582"
}

@inproceedings{wongkamjan-etal-2025-trust,
    title = "Should {I} Trust You? Detecting Deception in Negotiations using Counterfactual {RL}",
    author = "Wongkamjan, Wichayaporn  and
      Wang, Yanze  and
      Gu, Feng  and
      Peskoff, Denis  and
      Kummerfeld, Jonathan K.  and
      May, Jonathan  and
      Boyd-Graber, Jordan Lee",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1287/",
    doi = "10.18653/v1/2025.findings-acl.1287",
    pages = "25099--25113",
    ISBN = "979-8-89176-256-5"
}

@inproceedings{israeli-etal-2025-million,
    title = "The Million Authors Corpus: A Cross-Lingual and Cross-Domain {W}ikipedia Dataset for Authorship Verification",
    author = "Israeli, Abraham  and
      Liu, Shuai  and
      May, Jonathan  and
      Jurgens, David",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1335/",
    doi = "10.18653/v1/2025.findings-acl.1335",
    pages = "25997--26017",
    ISBN = "979-8-89176-256-5"
}


@inproceedings{cho-etal-2025-vision,
    title = "Can Vision Language Models Understand Mimed Actions?",
    author = "Cho, Hyundong Justin  and
      Lin, Spencer  and
      Srinivasan, Tejas  and
      Saxon, Michael  and
      Kwon, Deuksin  and
      Chavez, Natali T.  and
      May, Jonathan",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.1372/",
    doi = "10.18653/v1/2025.findings-acl.1372",
    pages = "26744--26759",
    ISBN = "979-8-89176-256-5"
}


@inproceedings{cho-etal-2025-tuning,
    title = "Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning",
    author = "Cho, Hyundong Justin  and
      Sharma, Karishma  and
      Jedema, Nicolaas Paul  and
      Ribeiro, Leonardo F. R.  and
      May, Jonathan  and
      Moschitti, Alessandro",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2025",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-naacl.326/",
    pages = "5864--5885",
    ISBN = "979-8-89176-195-7",
    abstract = "Language models are aligned to the collective voice of many, resulting in generic outputs that do not align with specific users' styles. In this work, we present Trial-Error-Explain In-Context Learning (TICL), a tuning-free method that personalizes language models for text generation tasks with fewer than 10 examples per user. TICL iteratively expands an in-context learning prompt via a trial-error-explain process, adding model-generated negative samples and explanations that provide fine-grained guidance towards a specific user`s style. TICL achieves favorable win rates on pairwise comparisons with LLM-as-a-judge up to 91.5{\%} against the previous state-of-the-art and outperforms competitive tuning-free baselines for personalized alignment tasks of writing emails, essays and news articles. Both lexical and qualitative analyses show that the negative samples and explanations enable language models to learn stylistic context more effectively and overcome the bias towards structural and formal phrases observed in their zero-shot outputs. By front-loading inference compute to create a user-specific in-context learning prompt that does not require extra generation steps at test time, presents a novel yet simple approach for personalized alignment."
}

@inproceedings{gu-etal-2025-personalized,
    title = "Personalized Help for Optimizing Low-Skilled Users' Strategy",
    author = "Gu, Feng  and
      Wongkamjan, Wichayaporn  and
      Boyd-Graber, Jordan Lee  and
      Kummerfeld, Jonathan K.  and
      Peskoff, Denis  and
      May, Jonathan",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-short.6/",
    pages = "65--74",
    ISBN = "979-8-89176-190-2",
    abstract = "AIs can beat humans in game environments; however, how helpful those agents are to human remains understudied. We augment Cicero, a natural language agent that demonstrates superhuman performance in Diplomacy, to generate both move and message advice based on player intentions. A dozen Diplomacy games with novice and experienced players, with varying advice settings, show that some of the generated advice is beneficial. It helps novices compete with experienced players and in some instances even surpass them. The mere presence of advice can be advantageous, even if players do not follow it."
}


@inproceedings{liu-may-2025-style,
    title = "Style Transfer with Multi-iteration Preference Optimization",
    author = "Liu, Shuai  and
      May, Jonathan",
    editor = "Chiruzzo, Luis  and
      Ritter, Alan  and
      Wang, Lu",
    booktitle = "Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = apr,
    year = "2025",
    address = "Albuquerque, New Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.naacl-long.135/",
    pages = "2663--2681",
    ISBN = "979-8-89176-189-6",
    abstract = "Numerous recent techniques for text style transfer characterize their approaches as variants of reinforcement learning and preference optimization. In this work, we consider the relationship between these approaches and a class of optimization approaches developed primarily for (non-neural) statistical machine translation, formerly known as {\textquoteleft}tuning'. Inspired by these techniques from the past, we improve upon established preference optimization approaches, incorporating multiple iterations of exploration and optimization, and choosing contrastive examples by following a {\textquoteleft}hope' vs {\textquoteleft}fear' sampling strategy. Cognizant of the difference between machine translation and style transfer, however, we further tailor our framework with a new pseudo-parallel data generation method and a dynamic weighted reward aggregation method to tackle the lack of parallel data and the need for a multi-objective reward. We evaluate our model on two commonly used text style transfer datasets. Through automatic and human evaluation results we show the effectiveness and the superiority of our model compared to state-of-the-art baselines."
}



@InProceedings{he-etal-2025-token,
  author =       {Shanxiu He and Mutasem Al-Darabsah and Suraj Nair and Jonathan May and Tarun Agarwal and Tao Yang and Choon Hui Teo},
  title =        {Token Pruning Optimization for Efficient Dense Retrieval with Multi-Vector Representations},
  booktitle = {Proc. ECIR},
  year =      2025,
  month =     {April},
  address =   {Lucca, Italy},
  url = {https://dl.acm.org/doi/10.1007/978-3-031-88708-6_7},}

@inproceedings{
ma2024megalodon,
title={Megalodon: Efficient {LLM} Pretraining and Inference with Unlimited Context Length},
author={Xuezhe Ma and Xiaomeng Yang and Wenhan Xiong and Beidi Chen and LILI YU and Hao Zhang and Jonathan May and Luke Zettlemoyer and Omer Levy and Chunting Zhou},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=XlAbMZu4Bo}
}


@misc{huang2024foodpuzzledevelopinglargelanguage,
      title={FoodPuzzle: Developing Large Language Model Agents as Flavor Scientists}, 
      author={Tenghao Huang and Donghee Lee and John Sweeney and Jiatong Shi and Emily Steliotes and Matthew Lange and Jonathan May and Muhao Chen},
      year={2024},
      eprint={2409.12832},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.12832}, 
}


@misc{liu2024authorshipstyletransferpolicy,
      title={Authorship Style Transfer with Policy Optimization}, 
      author={Shuai Liu and Shantanu Agarwal and Jonathan May},
      year={2024},
      eprint={2403.08043},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08043}, 
}


@inproceedings{cho-etal-2024-speechworthy,
    title = "Speechworthy Instruction-tuned Language Models",
    author = "Cho, Hyundong Justin  and
      Jedema, Nicolaas Paul  and
      Ribeiro, Leonardo F. R.  and
      Sharma, Karishma  and
      Szekely, Pedro  and
      Moschitti, Alessandro  and
      Janssen, Ruben  and
      May, Jonathan",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.595",
    doi = "10.18653/v1/2024.emnlp-main.595",
    pages = "10652--10670",
    abstract = "Current instruction-tuned language models are exclusively trained with textual preference data and thus may not be aligned to the unique requirements of other modalities, such as speech. To better align language models with the speech domain, we explore i) prompting strategies based on radio-industry best practices and ii) preference learning using a novel speech-based preference data of 20K samples collected by annotators who listen to response pairs. Both human and automatic evaluation show that both prompting and preference learning increase the speech-suitability of popular instruction tuned LLMs. More interestingly, we show that these methods are additive; combining them achieves the best win rates in head-to-head comparison, resulting in responses that are preferred or tied to the base model in 76.2{\%} of comparisons on average. Lastly, we share lexical, syntactical, and qualitative analyses that elicit how our studied methods differ with baselines in generating more speech-suitable responses.",
}


@inproceedings{tian-etal-2024-large-language,
    title = "Are Large Language Models Capable of Generating Human-Level Narratives?",
    author = "Tian, Yufei  and
      Huang, Tenghao  and
      Liu, Miri  and
      Jiang, Derek  and
      Spangher, Alexander  and
      Chen, Muhao  and
      May, Jonathan  and
      Peng, Nanyun",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.978",
    doi = "10.18653/v1/2024.emnlp-main.978",
    pages = "17659--17681",
    abstract = "As daily reliance on large language models (LLMs) grows, assessing their generation quality is crucial to understanding how they might impact on our communications. This paper investigates the capability of LLMs in storytelling, focusing on narrative development and plot progression. We introduce a novel computational framework to analyze narratives through three discourse-level aspects: i) story arcs, ii) turning points, and iii) affective dimensions, including arousal and valence. By leveraging expert and automatic annotations, we uncover significant discrepancies between the LLM- and human- written stories. While human-written stories are suspenseful, arousing, and diverse in narrative structures, LLM stories are homogeneously positive and lack tension. Next, we measure narrative reasoning skills as a precursor to generative capacities, concluding that most LLMs fall short of human abilities in discourse understanding. Finally, we show that explicit integration of aforementioned discourse features can enhance storytelling, as is demonstrated by over 40{\%} improvement in neural storytelling in terms of diversity, suspense, and arousal. Such advances promise to facilitate greater and more natural roles LLMs in human communication.",
}

@inproceedings{spangher-etal-2024-explaining,
    title = "Explaining Mixtures of Sources in News Articles",
    author = "Spangher, Alexander  and
      Youn, James  and
      DeButts, Matt  and
      Peng, Nanyun  and
      Ferrara, Emilio  and
      May, Jonathan",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.930",
    doi = "10.18653/v1/2024.findings-emnlp.930",
    pages = "15837--15859",
    abstract = "Human writers plan, {\_}then{\_} write. For large language models (LLMs) to play a role in longer-form article generation, we must understand the planning steps humans make before writing. We explore one kind of planning, source-selection in news, as a case-study for evaluating plans in long-form generation. We ask: why do {\_}specific{\_} stories call for {\_}specific{\_} kinds of sources? We imagine a generative process for story writing where a source-selection schema is first selected by a journalist, and then sources are chosen based on categories in that schema. Learning the article{'}s {\_}plan{\_} means predicting the schema initially chosen by the journalist. Working with professional journalists, we adapt five existing schemata and introduce three new ones to describe journalistic plans for the inclusion of sources in documents. Then, inspired by Bayesian latent-variable modeling, we develop metrics to select the most likely plan, or schema, underlying a story, which we use to compare schemata. We find that two schemata: {\_}stance{\_} and {\_}social affiliation{\_} best explain source plans in most documents. However, other schemata like {\_}textual entailment{\_} explain source plans in factually rich topics like {``}Science{''}. Finally, we find we can predict the most suitable schema given just the article{'}s headline with reasonable accuracy. We see this as an important case-study for human planning, and provides a framework and approach for evaluating other kinds of plans, like discourse or plot-oriented plans. We release a corpora, {\_}NewsSources{\_}, with annotations for 4M articles, for further study.",
}

@inproceedings{10.1145/3626772.3657812,
author = {M'hamdi, Meryem and May, Jonathan and Dernoncourt, Franck and Bui, Trung and Yoon, Seunghyun},
title = {Multilingual Meta-Distillation Alignment for Semantic Retrieval},
year = {2024},
isbn = {9798400704314},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3626772.3657812},
doi = {10.1145/3626772.3657812},
abstract = {Multilingual semantic retrieval involves retrieving semantically relevant content to a query irrespective of the language. Compared to monolingual and bilingual semantic retrieval, multilingual semantic retrieval requires a stronger alignment approach to pull the contents to be retrieved close to the representation of their corresponding queries, no matter their language combinations. Traditionally, this is achieved through more supervision in the form of multilingual parallel resources, which are expensive to obtain, especially for low-resource languages. In this work, on top of an optimization-based Model-Agnostic Meta-Learner (MAML), we propose a data-efficient meta-distillation approach: MAML-Align,1 specifically for low-resource multilingual semantic retrieval. Our approach simulates a gradual feedback loop from monolingual to bilingual and from bilingual to multilingual semantic retrieval. We systematically compare multilingual meta-distillation learning to different baselines and conduct ablation studies on the role of different sampling approaches in the meta-task construction. We show that MAML-Align's gradual feedback loop boosts the generalization to different languages, including zero-shot ones, better than naive fine-tuning and vanilla MAML.},
booktitle = {Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {597–607},
numpages = {11},
keywords = {knowledge distillation, maml, meta-distillation, meta-learning, multilingual representations, semantic retrieval},
location = {Washington DC, USA},
series = {SIGIR '24}
}

@article{He_May_Lerman_2024, title={CPL-NoViD: Context-Aware Prompt-Based Learning for Norm Violation Detection in Online Communities}, volume={18}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/31335}, DOI={10.1609/icwsm.v18i1.31335}, abstractNote={Detecting norm violations in online communities is critical to maintaining healthy and safe spaces for online discussions. Existing machine learning approaches often struggle to adapt to the diverse rules and interpretations across different communities due to the inherent challenges of fine-tuning models for such context-specific tasks. In this paper, we introduce Context-aware Prompt-based Learning for Norm Violation Detection (CPL-NoViD), a novel method that employs prompt-based learning to detect norm violations across various types of rules. CPL-NoViD outperforms the baseline by incorporating context through natural language prompts and demonstrates improved performance across different rule types. Significantly, it not only excels in cross-rule-type and cross-community norm violation detection but also exhibits adaptability in few-shot learning scenarios. Most notably, it establishes a new state-of-the-art in norm violation detection, surpassing existing benchmarks. Our work highlights the potential of prompt-based learning for context-sensitive norm violation detection and paves the way for future research on more adaptable, context-aware models to better support online community moderators.}, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={He, Zihao and May, Jonathan and Lerman, Kristina}, year={2024}, month={May}, pages={569-582} }


@inproceedings{spangher-etal-2024-tracking,
    title = "Tracking the Newsworthiness of Public Documents",
    author = "Spangher, Alexander  and
      Tumgoren, Serdar  and
      Welsh, Ben  and
      Peng, Nanyun  and
      Ferrara, Emilio  and
      May, Jonathan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.763",
    pages = "14150--14168",
    abstract = "Journalists regularly make decisions on whether or not to report stories, based on {``}news values{''}. In this work, we wish to explicitly model these decisions to explore {\_}when{\_} and {\_}why{\_} certain stories get press attention. This is challenging because very few labelled links between source documents and news articles exist and language use between corpora is very different. We address this problem by implementing a novel {\_}probabilistic relational modeling{\_} framework, which we show is a low-annotation linking methodology that outperforms other, more state-of-the-art retrieval-based baselines. Next, we define a new task: {\_}{\_}newsworthiness prediction{\_}{\_}, to predict if a policy item will get covered. We focus on news coverage of local public policy in the San Francisco Bay Area by the {\_}San Francisco Chronicle{\_}. We gather 15k policies discussed across 10 years of public policy meetings, and transcribe over 3,200 hours of public discussion. In general, we find limited impact of public discussion on newsworthiness prediction accuracy, suggesting that some of the most important stories barely get discussed in public.Finally, we show that newsworthiness predictions can be a useful assistive tool for journalists seeking to keep abreast of local government. We perform human evaluation with expert journalists and show our systems identify policies they consider newsworthy with 68{\%} F1 and our coverage recommendations are helpful with an 84{\%} win-rate against baseline. We release all code and data to our work here: https://github.com/alex2awesome/newsworthiness-public.",
}


@inproceedings{cho-etal-2024-boteval,
    title = "{B}ot{E}val: Facilitating Interactive Human Evaluation",
    author = "Cho, Hyundong  and
      Gowda, Thamme  and
      Huang, Yuyang  and
      Lu, Zixun  and
      Tong, Tianli  and
      May, Jonathan",
    editor = "Cao, Yixin  and
      Feng, Yang  and
      Xiong, Deyi",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-demos.11/",
    doi = "10.18653/v1/2024.acl-demos.11",
    pages = "107--116",
    abstract = "Following the rapid progress in natural language processing (NLP) models, language models are applied to increasingly more complex interactive tasks such as negotiations and conversation moderations. Having human evaluators directly interact with these NLP models is essential for adequately evaluating the performance on such interactive tasks. We develop BotEval, an easily customizable, open-source, evaluation toolkit that focuses on enabling human-bot interactions as part of the evaluation process, as opposed to human evaluators making judgements for a static input. BotEval balances flexibility for customization and user-friendliness by providing templates for common use cases that span various degrees of complexity and built-in compatibility with popular crowdsourcing platforms.We showcase the numerous useful features of BotEval through a study that evaluates the performance of various chatbots on their effectiveness for conversational moderation and discuss how BotEval differs from other annotation tools."
}

@inproceedings{felkner-etal-2024-gpt,
    title = "{GPT} is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction",
    author = "Felkner, Virginia  and
      Thompson, Jennifer  and
      May, Jonathan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.760",
    pages = "14104--14115",
    abstract = "Social biases in LLMs are usually measured via bias benchmark datasets. Current benchmarks have limitations in scope, grounding, quality, and human effort required. Previous work has shown success with a community-sourced, rather than crowd-sourced, approach to benchmark development. However, this work still required considerable effort from annotators with relevant lived experience. This paper explores whether an LLM (specifically, GPT-3.5-Turbo) can assist with the task of developing a bias benchmark dataset from responses to an open-ended community survey. We also extend the previous work to a new community and set of biases: the Jewish community and antisemitism. Our analysis shows that GPT-3.5-Turbo has poor performance on this annotation task and produces unacceptable quality issues in its output. Thus, we conclude that GPT-3.5-Turbo is not an appropriate substitute for human annotation in sensitive tasks related to social biases, and that its use actually negates many of the benefits of community-sourcing bias benchmarks.",
}

@inproceedings{wongkamjan-etal-2024-victories,
    title = "More Victories, Less Cooperation: Assessing Cicero{'}s Diplomacy Play",
    author = "Wongkamjan, Wichayaporn  and
      Gu, Feng  and
      Wang, Yanze  and
      Hermjakob, Ulf  and
      May, Jonathan  and
      Stewart, Brandon  and
      Kummerfeld, Jonathan  and
      Peskoff, Denis  and
      Boyd-Graber, Jordan",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.672",
    pages = "12423--12441",
    abstract = "The boardgame Diplomacy is a challenging setting for communicative and cooperative artificial intelligence. The most prominent communicative Diplomacy AI, Cicero, has excellent strategic abilities, exceeding human players. However, the best Diplomacy players master communication, not just tactics, which is why the game has received attention as an AI challenge. This work seeks to understand the degree to which Cicero succeeds at communication. First, we annotate in-game communication with abstract meaning representation to separate in-game tactics from general language. Second, we run two dozen games with humans and Cicero, totaling over 200 human-player hours of competition. While AI can consistently outplay human players, AI-Human communication is still limited because of AI{'}s difficulty with deception and persuasion. This shows that Cicero relies on strategy and has not yet reached the full promise of communicative and cooperative AI.",
}




@Article{may-etal-2024-antisemitism,
  author =       {Jonathan May and Virginia K. Felkner and Jennifer Thompson},
  title =        {We Can Have {AI} without Antisemitism---If We Want It},
  journal =      {AJS Perspectives},
  year =         2024,
  pages =     {36--38},
  month =     {Summer}}

@inproceedings{spangher-etal-2024-legaldiscourse,
    title = "{L}egal{D}iscourse: Interpreting When Laws Apply and To Whom",
    author = "Spangher, Alexander  and
      Xue, Zihan  and
      Wu, Te-Lin  and
      Hansen, Mark  and
      May, Jonathan",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.472",
    doi = "10.18653/v1/2024.naacl-long.472",
    pages = "8536--8559",
    abstract = "While legal AI has made strides in recent years, it still struggles with basic legal concepts: {\_}when{\_} does a law apply? {\_}Who{\_} does it applies to? {\_}What{\_} does it do? We take a {\_}discourse{\_} approach to addressing these problems and introduce a novel taxonomy for span-and-relation parsing of legal texts. We create a dataset, {\_}LegalDiscourse{\_} of 602 state-level law paragraphs consisting of 3,715 discourse spans and 1,671 relations. Our trained annotators have an agreement-rate $\kappa>.8$, yet few-shot GPT3.5 performs poorly at span identification and relation classification. Although fine-tuning improves performance, GPT3.5 still lags far below human level. We demonstrate the usefulness of our schema by creating a web application with journalists. We collect over 100,000 laws for 52 U.S. states and territories using 20 scrapers we built, and apply our trained models to 6,000 laws using U.S. Census population numbers. We describe two journalistic outputs stemming from this application: (1) an investigation into the increase in liquor licenses following population growth and (2) a decrease in applicable laws under different under-count projections.",
}

@inproceedings{mhamdi-may-2024-leitner,
    title = "Leitner-Guided Memory Replay for Cross-lingual Continual Learning",
    author = "M{'}hamdi, Meryem  and
      May, Jonathan",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.432",
    doi = "10.18653/v1/2024.naacl-long.432",
    pages = "7808--7821",
    abstract = "Cross-lingual continual learning aims to continuously fine-tune a downstream model on emerging data from new languages. One major challenge in cross-lingual continual learning is catastrophic forgetting: a stability-plasticity dilemma, where performance on previously seen languages decreases as the model learns to transfer to new languages. Experience replay, which revisits data from a fixed-size memory of old languages while training on new ones, is among the most successful approaches for solving this dilemma. Faced with the challenge of dynamically storing the memory with high-quality examples while complying with its fixed size limitations, we consider Leitner queuing, a human-inspired spaced-repetition technique, to determine what should be replayed at each phase of learning. Via a controlled set of quantitative and qualitative analyses across different memory strategies, we show that, just like humans, carefully picking informative examples to be prioritized in cross-lingual memory replay helps tame the stability-plasticity dilemma. Compared to vanilla and strong memory replay baselines, our Leitner-guided approach significantly and consistently decreases forgetting while maintaining accuracy across natural language understanding tasks, language orders, and languages.",
}

@inproceedings{cho-etal-2024-language,
    title = "Can Language Model Moderators Improve the Health of Online Discourse?",
    author = "Cho, Hyundong  and
      Liu, Shuai  and
      Shi, Taiwei  and
      Jain, Darpan  and
      Rizk, Basem  and
      Huang, Yuyang  and
      Lu, Zixun  and
      Wen, Nuan  and
      Gratch, Jonathan  and
      Ferrara, Emilio  and
      May, Jonathan",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-long.415",
    doi = "10.18653/v1/2024.naacl-long.415",
    pages = "7478--7496",
    abstract = "Conversational moderation of online communities is crucial to maintaining civility for a constructive environment, but it is challenging to scale and harmful to moderators. The inclusion of sophisticated natural language generation modules as a force multiplier to aid human moderators is a tantalizing prospect, but adequate evaluation approaches have so far been elusive. In this paper, we establish a systematic definition of conversational moderation effectiveness grounded on moderation literature and establish design criteria for conducting realistic yet safe evaluation. We then propose a comprehensive evaluation framework to assess models{'} moderation capabilities independently of human intervention. With our framework, we conduct the first known study of language models as conversational moderators, finding that appropriately prompted models that incorporate insights from social science can provide specific and fair feedback on toxic behavior but struggle to influence users to increase their levels of respect and cooperation.",
}

@inproceedings{jin-etal-2023-challenges,
    title = "Challenges in Context-Aware Neural Machine Translation",
    author = "Jin, Linghao  and
      He, Jacqueline  and
      May, Jonathan  and
      Ma, Xuezhe",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.943",
    doi = "10.18653/v1/2023.emnlp-main.943",
    pages = "15246--15263",
    abstract = "Context-aware neural machine translation, a paradigm that involves leveraging information beyond sentence-level context to resolve inter-sentential discourse dependencies and improve document-level translation quality, has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most context-aware translation models show only modest improvements over sentence-level systems. In this work, we investigate and present several core challenges that impede progress within the field, relating to discourse phenomena, context usage, model architectures, and document-level evaluation. To address these problems, we propose a more realistic setting for document-level translation, called paragraph-to-paragraph (PARA2PARA) translation, and collect a new dataset of Chinese-English novels to promote future research.",
}

@inproceedings{cho-etal-2023-continual,
    title = "Continual Dialogue State Tracking via Example-Guided Question Answering",
    author = "Cho, Hyundong  and
      Madotto, Andrea  and
      Lin, Zhaojiang  and
      Chandu, Khyathi  and
      Kottur, Satwik  and
      Xu, Jing  and
      May, Jonathan  and
      Sankar, Chinnadhurai",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.235",
    doi = "10.18653/v1/2023.emnlp-main.235",
    pages = "3873--3886",
    abstract = "Dialogue systems are frequently updated to accommodate new services, but naively updating them by continually training with data for new services in diminishing performance on previously learnt services. Motivated by the insight that dialogue state tracking (DST), a crucial component of dialogue systems that estimates the user{'}s goal as a conversation proceeds, is a simple natural language understanding task, we propose reformulating it as a bundle of granular example-guided question answering tasks to minimize the task shift between services and thus benefit continual learning. Our approach alleviates service-specific memorization and teaches a model to contextualize the given question and example to extract the necessary information from the conversation. We find that a model with just 60M parameters can achieve a significant boost by learning to learn from in-context examples retrieved by a retriever trained to identify turns with similar dialogue state changes. Combining our method with dialogue-level memory replay, our approach attains state of the art performance on DST continual learning metrics without relying on any complex regularization or parameter expansion methods.",
}

@inproceedings{moon-etal-2023-analyzing,
    title = "Analyzing Norm Violations in Live-Stream Chat",
    author = "Moon, Jihyung  and
      Lee, Dong-Ho  and
      Cho, Hyundong  and
      Jin, Woojeong  and
      Park, Chan  and
      Kim, Minwoo  and
      May, Jonathan  and
      Pujara, Jay  and
      Park, Sungjoon",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.55",
    doi = "10.18653/v1/2023.emnlp-main.55",
    pages = "852--868",
    abstract = "Toxic language, such as hate speech, can deter users from participating in online communities and enjoying popular platforms. Previous approaches to detecting toxic language and norm violations have been primarily concerned with conversations from online forums and social media, such as Reddit and Twitter. These approaches are less effective when applied to conversations on live-streaming platforms, such as Twitch and YouTube Live, as each comment is only visible for a limited time and lacks a thread structure that establishes its relationship with other comments. In this work, we share the first NLP study dedicated to detecting norm violations in conversations on live-streaming platforms. We define norm violation categories in live-stream chats and annotate 4,583 moderated comments from Twitch. We articulate several facets of live-stream data that differ from other forums, and demonstrate that existing models perform poorly in this setting. By conducting a user study, we identify the informational context humans use in live-stream moderation, and train models leveraging context to identify norm violations. Our results show that appropriate contextual information can boost moderation performance by 35{\%}.",
}

@inproceedings{spangher-etal-2023-identifying,
    title = "Identifying Informational Sources in News Articles",
    author = "Spangher, Alexander  and
      Peng, Nanyun  and
      Ferrara, Emilio  and
      May, Jonathan",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.emnlp-main.221",
    doi = "10.18653/v1/2023.emnlp-main.221",
    pages = "3626--3639",
    abstract = "News articles are driven by the informational sources journalists use in reporting. Modeling when, how and why sources get used together in stories can help us better understand the information we consume and even help journalists with the task of producing it. In this work, we take steps toward this goal by constructing the largest and widest-ranging annotated dataset, to date, of informational sources used in news writing. We first show that our dataset can be used to train high-performing models for information detection and source attribution. Then, we introduce a novel task, source prediction, to study the compositionality of sources in news articles {--} i.e. how they are chosen to complement each other. We show good modeling performance on this task, indicating that there is a pattern to the way different sources are used \textit{together} in news storytelling. This insight opens the door for a focus on sources in narrative science (i.e. planning-based language generation) and computational journalism (i.e. a source-recommendation system to aid journalists writing stories). All data and model code can be found at https://github.com/alex2awesome/source-exploration.",
}

@inproceedings{10.1007/978-3-031-43129-6_9,
author = {Chang, Rong-Ching and May, Jonathan and Lerman, Kristina},
title = {Feedback Loops and&nbsp;Complex Dynamics of&nbsp;Harmful Speech in&nbsp;Online Discussions},
year = {2023},
isbn = {978-3-031-43128-9},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-031-43129-6_9},
doi = {10.1007/978-3-031-43129-6_9},
abstract = {Harmful and toxic speech contribute to an unwelcoming online environment that suppresses participation and conversation. Efforts have focused on detecting and mitigating harmful speech; however, the mechanisms by which toxicity degrades online discussions are not well understood. This paper makes two contributions. First, to comprehensively model harmful comments, we introduce a multilingual misogyny and sexist speech detection model (). Second, we model the complex dynamics of online discussions as feedback loops in which harmful comments lead to negative emotions which prompt even more harmful comments. To quantify the feedback loops, we use a combination of mutual Granger causality and regression to analyze discussions on two political forums on Reddit: the moderated political forum r/Politics and the moderated neutral political forum r/NeutralPolitics. Our results suggest that harmful comments and negative emotions create self-reinforcing feedback loops in forums. Contrarily, moderation with neutral discussion appears to tip interactions into self-extinguishing feedback loops that reduce harmful speech and negative emotions. Our study sheds more light on the complex dynamics of harmful speech and the role of moderation and neutral discussion in mitigating these dynamics.},
booktitle = {Social, Cultural, and Behavioral Modeling: 16th International Conference, SBP-BRiMS 2023, Pittsburgh, PA, USA, September 20–22, 2023, Proceedings},
pages = {85–94},
numpages = {10},
keywords = {Feedback Loop, Moderation, Granger Causality},
location = {Pittsburgh, PA, USA}
}

@inproceedings{spangher23.djc,
  author = {Alexander Spangher and James Youn and Jonathan May and Nanyun Peng},
  title =        {First Steps Towards a Source Recommendation Engine:
Investigating How Sources Are Used in News Articles},
  year =         2023,
  url={https://www.datajconf.com/papers/CJ_DataJConf_2023_paper_74.pdf},
  booktitle = {Proc. The Joint Computation + Journalism European Data \& Computational Journalism Conference},
  address =   {Zurich, Switzerland},
  month =     {June}}

@inproceedings{10.1145/3543873.3587629,
author = {Bonab, Hamed and Joshi, Ashutosh and Bhatia, Ravi and Gandhi, Ankit and Huddar, Vijay and Naik, Juhi and Al-Darabsah, Mutasem and Teo, Choon Hui and May, Jonathan and Agarwal, Tarun and Petricek, Vaclav},
title = {Blend and Match: Distilling Semantic Search Models with Different Inductive Biases and Model Architectures},
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543873.3587629},
doi = {10.1145/3543873.3587629},
abstract = {Commercial search engines use different semantic models to augment lexical matches. These models provide candidate items for a user’s query from a target space of millions to billions of items. Models with different inductive biases provide relatively different predictions, making it desirable to launch multiple semantic models in production. However, latency and resource constraints make simultaneously deploying multiple models impractical. In this paper, we introduce a distillation approach, called Blend and Match (BM), to unify two different semantic search models into a single model. We use a Bi-encoder semantic matching model as our primary model and propose a novel loss function to incorporate eXtreme Multi-label Classification (XMC) predictions as the secondary model. Our experiments conducted on two large-scale datasets, collected from a popular e-commerce store, show that our proposed approach significantly improves the recall of the primary Bi-encoder model by 11\% to 17\% with a minimal loss in precision. We show that traditional knowledge distillation approaches result in a sub-optimal performance for our problem setting, and our BM approach yields comparable rankings with strong Rank Fusion (RF) methods used only if one could deploy multiple models.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {869–877},
numpages = {9},
keywords = {Semantic Search, Ranking Distillation, Product Search, Model Blending},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}


@inproceedings{yu-etal-2023-bridging,
    title = "Bridging the Gap between Native Text and Translated Text through Adversarial Learning: A Case Study on Cross-Lingual Event Extraction",
    author = "Yu, Pengfei  and
      May, Jonathan  and
      Ji, Heng",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.57",
    doi = "10.18653/v1/2023.findings-eacl.57",
    pages = "754--769",
    abstract = "Recent research in cross-lingual learning has found that combining large-scale pretrained multilingual language models with machine translation can yield good performance. We explore this idea for cross-lingual event extraction with a new model architecture that jointly encodes a source language input sentence with its translation to the target language during training, and takes a target language sentence with its translation back to the source language as input during evaluation. However, we observe significant representational gap between the native source language texts during training and the texts translated into source language during evaluation, as well as the texts translated into target language during training and the native target language texts during evaluation. This representational gap undermines the effectiveness of cross-lingual transfer learning for event extraction with machine-translated data. In order to mitigate this problem, we propose an adversarial training framework that encourages the language model to produce more similar representations for the translated text and the native text. To be specific, we train the language model such that its hidden representations are able to fool a jointly trained discriminator that distinguishes translated texts{'} representations from native texts{'} representations. We conduct experiments on cross-lingual for event extraction across three languages. Results demonstrate that our proposed adversarial training can effectively incorporate machine translation to improve event extraction, while simply adding machine-translated data yields unstable performance due to the representational gap.",
}

@inproceedings{liu-etal-2023-recap,
    title = "{RECAP}: Retrieval-Enhanced Context-Aware Prefix Encoder for Personalized Dialogue Response Generation",
    author = "Liu, Shuai  and
      Cho, Hyundong  and
      Freedman, Marjorie  and
      Ma, Xuezhe  and
      May, Jonathan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.468",
    doi = "10.18653/v1/2023.acl-long.468",
    pages = "8404--8419",
    abstract = "Endowing chatbots with a consistent persona is essential to an engaging conversation, yet it remains an unresolved challenge. In this work, we propose a new retrieval-enhanced approach for personalized response generation. Specifically, we design a hierarchical transformer retriever trained on dialogue domain data to perform personalized retrieval and a context-aware prefix encoder that fuses the retrieved information to the decoder more effectively. Extensive experiments on a real-world dataset demonstrate the effectiveness of our model at generating more fluent and personalized responses. We quantitatively evaluate our model{'}s performance under a suite of human and automatic metrics and find it to be superior compared to state-of-the-art baselines on English Reddit conversations.",
}

@inproceedings{
ma2023mega,
title={Mega: Moving Average Equipped Gated Attention},
author={Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=qNLe3iq2El}
}

@inproceedings{cho-etal-2022-know,
    title = "Know Thy Strengths: Comprehensive Dialogue State Tracking Diagnostics",
    author = "Cho, Hyundong  and
      Sankar, Chinnadhurai  and
      Lin, Christopher  and
      Sadagopan, Kaushik  and
      Shayandeh, Shahin  and
      Celikyilmaz, Asli  and
      May, Jonathan  and
      Beirami, Ahmad",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.391",
    pages = "5345--5359",
    abstract = "Recent works that revealed the vulnerability of dialogue state tracking (DST) models to distributional shifts have made holistic comparisons on robustness and qualitative analyses increasingly important for understanding their relative performance. We present our findings from standardized and comprehensive DST diagnoses, which have previously been sparse and uncoordinated, using our toolkit, CheckDST, a collection of robustness tests and failure mode analytics. We discover that different classes of DST models have clear strengths and weaknesses, where generation models are more promising for handling language variety while span-based classification models are more robust to unseen entities. Prompted by this discovery, we also compare checkpoints from the same model and find that the standard practice of selecting checkpoints using validation loss/accuracy is prone to overfitting and each model class has distinct patterns of failure. Lastly, we demonstrate how our diagnoses motivate a pre-finetuning procedure with non-dialogue data that offers comprehensive improvements to generation models by alleviating the impact of distributional shifts through transfer learning.",
}


@InProceedings{10.1007/978-3-031-43129-6_5,
author="Chen, Kai and He, Zihao and Chang, Rong-Ching and May, Jonathan and Lerman, Kristina",
editor="Thomson, Robert and Al-khateeb, Samer and Burger, Annetta
and Park, Patrick
and A. Pyke, Aryn",
title="Anger Breeds Controversy: Analyzing Controversy and¬†Emotions on¬†Reddit",
booktitle="Social, Cultural, and Behavioral Modeling",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="44--53",
url = {https://arxiv.org/abs/2212.00339},
abstract="Emotions play an important role in interpersonal interactions and social conflict, yet their function in the development of controversy and disagreement in online conversations has not been fully explored. To address this gap, we study controversy on Reddit, a popular network of online discussion forums. We collect discussions from various topical forums and use emotion detection to recognize a range of emotions from text, including anger, fear, joy, admiration, etc. (Code and dataset are publicly available at https://github.com/ChenK7166/controversy-emotion). We find controversial comments express more anger and less admiration, joy, and optimism than non-controversial comments. Moreover, controversial comments affect emotions of downstream comments, resulting in a long-term increase in anger and a decrease in positive emotions. The magnitude and direction of emotional change differ by forum. Finally, we show that emotions help better predict which comments will become controversial. Understanding the dynamics of emotions in online discussions can help communities to manage conversations better.",
isbn="978-3-031-43129-6"
}


@misc{https://doi.org/10.48550/arxiv.2210.05096,
  doi = {10.48550/ARXIV.2210.05096},
  url = {https://arxiv.org/abs/2210.05096},
  author = {Gowda, Thamme and Gheini, Mozhdeh and May, Jonathan},
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Checks and Strategies for Enabling Code-Switched Machine Translation},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2209.10655,
  doi = {10.48550/ARXIV.2209.10655},
  url = {https://arxiv.org/abs/2209.10655},
  author = {Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Mega: Moving Average Equipped Gated Attention},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{yu-etal-2022-building,
    title = "Building an Event Extractor with Only a Few Examples",
    author = "Yu, Pengfei  and
      Zhang, Zixuan  and
      Voss, Clare  and
      May and Heng Ji, Jonathan",
    booktitle = "Proceedings of the Third Workshop on Deep Learning for Low-Resource Natural Language Processing",
    month = jul,
    year = "2022",
    address = "Hybrid",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.deeplo-1.11",
    pages = "102--109",
    abstract = "t",
}

@inproceedings{joshi-etal-2022-augmenting,
    title = "Augmenting Training Data for Massive Semantic Matching Models in Low-Traffic {E}-commerce Stores",
    author = "Joshi, Ashutosh  and
      Vishwanath, Shankar  and
      Teo, Choon  and
      Petricek, Vaclav  and
      Vishwanathan, Vishy  and
      Bhagat, Rahul  and
      May, Jonathan",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track",
    month = jul,
    year = "2022",
    address = "Hybrid: Seattle, Washington + Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-industry.19",
    pages = "160--167",
    abstract = "Extreme multi-label classification (XMC) systems have been successfully applied in e-commerce (Shen et al., 2020; Dahiya et al., 2021) for retrieving products based on customer behavior. Such systems require large amounts of customer behavior data (e.g. queries, clicks, purchases) for training. However, behavioral data is limited in low-traffic e-commerce stores, impacting performance of these systems. In this paper, we present a technique that augments behavioral training data via query reformulation. We use the Aggregated Label eXtreme Multi-label Classification (AL-XMC) system (Shen et al., 2020) as an example semantic matching model and show via crowd-sourced human judgments that, when the training data is augmented through query reformulations, the quality of AL-XMC improves over a baseline that does not use query reformulation. We also show in online A/B tests that our method significantly improves business metrics for the AL-XMC model.",
}


@misc{https://doi.org/10.48550/arxiv.2112.08321,
  doi = {10.48550/ARXIV.2112.08321},
  url = {https://arxiv.org/abs/2112.08321},
  author = {Cho, Hyundong and Sankar, Chinnadhurai and Lin, Christopher and Sadagopan, Kaushik Ram and Shayandeh, Shahin and Celikyilmaz, Asli and May, Jonathan and Beirami, Ahmad},
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {CheckDST: Measuring Real-World Generalization of Dialogue State Tracking Performance},
  publisher = {arXiv},
  year = {2021},
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{chawla-etal-2022-opponent,
    title = "Opponent Modeling in Negotiation Dialogues by Related Data Adaptation",
    author = "Chawla, Kushal  and
      Lucas, Gale  and
      May, Jonathan  and
      Gratch, Jonathan",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2022",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-naacl.50",
    pages = "661--674",
    abstract = "Opponent modeling is the task of inferring another party{'}s mental state within the context of social interactions. In a multi-issue negotiation, it involves inferring the relative importance that the opponent assigns to each issue under discussion, which is crucial for finding high-value deals. A practical model for this task needs to infer these priorities of the opponent on the fly based on partial dialogues as input, without needing additional annotations for training. In this work, we propose a ranker for identifying these priorities from negotiation dialogues. The model takes in a partial dialogue as input and predicts the priority order of the opponent. We further devise ways to adapt related data sources for this task to provide more explicit supervision for incorporating the opponent{'}s preferences and offers, as a proxy to relying on granular utterance-level annotations. We show the utility of our proposed approach through extensive experiments based on two dialogue datasets. We find that the proposed data adaptations lead to strong performance in zero-shot and few-shot scenarios. Moreover, they allow the model to perform better than baselines while accessing fewer utterances from the opponent. We release our code to support future work in this direction.",
}



@inproceedings{mhamdi-etal-2023-cross,
    title = "Cross-lingual Continual Learning",
    author = "M{'}hamdi, Meryem  and
      Ren, Xiang  and
      May, Jonathan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.217",
    doi = "10.18653/v1/2023.acl-long.217",
    pages = "3908--3943",
    abstract = "The longstanding goal of multi-lingual learning has been to develop a universal cross-lingual model that can withstand the changes in multi-lingual data distributions. There has been a large amount of work to adapt such multi-lingual models to unseen target languages. However, the majority of work in this direction focuses on the standard one-hop transfer learning pipeline from source to target languages, whereas in realistic scenarios, new languages can be incorporated at any time in a sequential manner. In this paper, we present a principled Cross-lingual Continual Learning (CCL) evaluation paradigm, where we analyze different categories of approaches used to continually adapt to emerging data from different languages. We provide insights into what makes multilingual sequential learning particularly challenging. To surmount such challenges, we benchmark a representative set of cross-lingual continual learning algorithms and analyze their knowledge preservation, accumulation, and generalization capabilities compared to baselines on carefully curated datastreams. The implications of this analysis include a recipe for how to measure and balance different cross-lingual continual learning desiderata, which go beyond conventional transfer learning.",
}

@inproceedings{gheini-etal-2023-know,
    title = "Know Where You{'}re Going: Meta-Learning for Parameter-Efficient Fine-Tuning",
    author = "Gheini, Mozhdeh  and
      Ma, Xuezhe  and
      May, Jonathan",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.737",
    doi = "10.18653/v1/2023.findings-acl.737",
    pages = "11602--11612",
    abstract = "A recent family of techniques, dubbed lightweight fine-tuning methods, facilitates parameter-efficient transfer by updating only a small set of additional parameters while keeping the parameters of the original model frozen. While proven to be an effective approach, there are no existing studies on if and how such knowledge of the downstream fine-tuning approach calls for complementary measures after pre-training and before fine-tuning. In this work, we show that taking the ultimate choice of fine-tuning into consideration boosts the performance of parameter-efficient fine-tuning. By relying on optimization-based meta-learning using MAML with certain modifications for our distinct purpose, we prime the pre-trained model specifically for parameter-efficient fine-tuning, resulting in gains of up to 4.96 points on cross-lingual NER fine-tuning. Our ablation settings and analyses further reveal that the specific approach we take to meta-learning is crucial for the attained gains.",
}

@inproceedings{bremerman-etal-2022-machine,
    title = "Machine Translation Robustness to Natural Asemantic Variation",
    author = "Bremerman, Jacob  and
      Ren, Xiang  and
      May, Jonathan",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.230",
    pages = "3517--3532",
    abstract = "Current Machine Translation (MT) models still struggle with more challenging input, such as noisy data and tail-end words and phrases. Several works have addressed this robustness issue by identifying specific categories of noise and variation then tuning models to perform better on them. An important yet under-studied category involves minor variations in nuance (non-typos) that preserve meaning w.r.t. the target language. We introduce and formalize this category as Natural Asemantic Variation (NAV) and investigate it in the context of MT robustness. We find that existing MT models fail when presented with NAV data, but we demonstrate strategies to improve performance on NAV by fine-tuning them with human-generated variations. We also show that NAV robustness can be transferred across languages and find that synthetic perturbations can achieve some but not all of the benefits of organic NAV data.",
}


@inproceedings{spangher-etal-2022-newsedits,
    title = "{N}ews{E}dits: A News Article Revision Dataset and a Novel Document-Level Reasoning Challenge",
    author = "Spangher, Alexander  and
      Ren, Xiang  and
      May, Jonathan  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.10",
    pages = "127--157",
    abstract = "News article revision histories provide clues to narrative and factual evolution in news articles. To facilitate analysis of this evolution, we present the first publicly available dataset of news revision histories, NewsEdits. Our dataset is large-scale and multilingual; it contains 1.2 million articles with 4.6 million versions from over 22 English- and French-language newspaper sources based in three countries, spanning 15 years of coverage (2006-2021).We define article-level edit actions: Addition, Deletion, Edit and Refactor, and develop a high-accuracy extraction algorithm to identify these actions. To underscore the factual nature of many edit actions, we conduct analyses showing that added and deleted sentences are more likely to contain updating events, main content and quotes than unchanged sentences. Finally, to explore whether edit actions are predictable, we introduce three novel tasks aimed at predicting actions performed during version updates. We show that these tasks are possible for expert humans but are challenging for large NLP models. We hope this can spur research in narrative framing and help provide predictive tools for journalists chasing breaking news.",
}


@inproceedings{sun-etal-2022-investigating,
    title = "Investigating the Benefits of Free-Form Rationales",
    author = "Sun, Jiao  and
      Swayamdipta, Swabha  and
      May, Jonathan  and
      Ma, Xuezhe",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.432",
    pages = "5867--5882",
    abstract = "Free-form rationales aim to aid model interpretability by supplying the background knowledge that can help understand model decisions. Crowdsourced rationales are provided for commonsense QA instances in popular datasets such as CoS-E and ECQA, but their utility remains under-investigated. We present human studies which show that ECQA rationales indeed provide additional background information to understand a decision, while over 88{\%} of CoS-E rationales do not. Inspired by this finding, we ask: can the additional context provided by free-form rationales benefit models, similar to human users? We investigate the utility of rationales as an additional source of supervision, by varying the quantity and quality of rationales during training. After controlling for instances where rationales leak the correct answer while not providing additional background knowledge, we find that incorporating only 5{\%} of rationales during training can boost model performance by 47.22{\%} for CoS-E and 57.14{\%} for ECQA during inference. Moreover, we also show that rationale quality matters: compared to crowdsourced rationales, T5-generated rationales provide not only weaker supervision to models, but are also not helpful for humans in aiding model interpretability.",
}

@inproceedings{felkner-etal-2023-winoqueer,
    title = "{W}ino{Q}ueer: A Community-in-the-Loop Benchmark for Anti-{LGBTQ}+ Bias in Large Language Models",
    author = "Felkner, Virginia  and
      Chang, Ho-Chun Herbert  and
      Jang, Eugene  and
      May, Jonathan",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.507",
    doi = "10.18653/v1/2023.acl-long.507",
    pages = "9126--9140",
    abstract = "We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.",
}


@misc{https://doi.org/10.48550/arxiv.2206.11484,
  doi = {10.48550/ARXIV.2206.11484},
  url = {https://arxiv.org/abs/2206.11484},
  author = {Felkner, Virginia K. and Chang, Ho-Chun Herbert and Jang, Eugene and May, Jonathan},
  keywords = {Computation and Language (cs.CL), Computers and Society (cs.CY), FOS: Computer and information sciences, FOS: Computer and information sciences, I.2.7},
  title = {Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models},
  publisher = {arXiv},
  year = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{aldarrab-may-2022-segmenting,
    title = "Segmenting Numerical Substitution Ciphers",
    author = "Aldarrab, Nada  and
      May, Jonathan",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.44",
    pages = "706--714",
    abstract = "Deciphering historical substitution ciphers is a challenging problem. Example problems that have been previously studied include detecting cipher type, detecting plaintext language, and acquiring the substitution key for segmented ciphers. However, attacking unsegmented ciphers is still a challenging task. Segmentation (i.e. finding substitution units) is essential for cracking those ciphers. In this work, we propose the first automatic methods to segment those ciphers using Byte Pair Encoding (BPE) and unigram language models. Our methods achieve an average segmentation error of 2{\%} on 100 randomly-generated monoalphabetic ciphers and 27{\%} on 3 real historical homophonic ciphers. We also propose a method for solving non-deterministic ciphers with existing keys using a lattice and a pretrained language model. Our method leads to the full solution of the IA cipher; a real historical cipher that has not been fully solved until this work.",
}


@article{DBLP:journals/corr/abs-2104-10263,
  author    = {Alexander Spangher and
               Jonathan May},
  title     = {StateCensusLaws.org: A Web Application
               for Consuming and Annotating Legal Discourse Learning},
  journal   = {CoRR},
  volume    = {abs/2104.10263},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.10263},
  eprinttype = {arXiv},
  eprint    = {2104.10263},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-10263.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2108-11063,
  author    = {Hyundong Cho and
               Basel Shbita and
               Kartik Shenoy and
               Shuai Liu and
               Nikhil Patel and
               Hitesh Pindikanti and
               Jennifer Lee and
               Jonathan May},
  title     = {Viola: {A} Topic Agnostic Generate-and-Rank Dialogue System},
  journal   = {CoRR},
  volume    = {abs/2108.11063},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.11063},
  eprinttype = {arXiv},
  eprint    = {2108.11063},
  timestamp = {Fri, 27 Aug 2021 15:02:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-11063.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{mirzaalian2021explaining,
      title={Explaining Face Presentation Attack Detection Using Natural Language}, 
      author={Hengameh Mirzaalian and Mohamed E. Hussein and Leonidas Spinoulas and Jonathan May and Wael Abd-Almageed},
      year={2021},
      eprint={2111.04862},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


@inproceedings{10.1145/3465413.3488575,
author = {Weideman, Nicolaas and Felkner, Virginia K. and Wu, Wei-Cheng and May, Jonathan and Hauser, Christophe and Garcia, Luis},
title = {PERFUME: Programmatic Extraction and Refinement for Usability of Mathematical Expression},
year = {2021},
isbn = {9781450385527},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3465413.3488575},
doi = {10.1145/3465413.3488575},
abstract = {Algorithmic identification is the crux for several binary analysis applications, including malware analysis, vulnerability discovery, and embedded firmware reverse engineering. However, data-driven and signature-based approaches often break down when encountering outlier realizations of a particular algorithm. Moreover, reverse engineering of domain-specific binaries often requires collaborative analysis between reverse engineers and domain experts. Communicating the behavior of an unidentified binary program to non-reverse engineers necessitates the recovery of algorithmic semantics in a human-digestible form. This paper presents PERFUME, a framework that extracts symbolic math expressions from low-level binary representations of an algorithm. PERFUME works by translating a symbolic output representation of a binary function to a high-level mathematical expression. In particular, we detail how source and target representations are generated for training a machine translation model. We integrate PERFUME as a plug-in for Ghidra--an open-source reverse engineering framework. We present our preliminary findings for domain-specific use cases and formalize open challenges in mathematical expression extraction from algorithmic implementations.},
booktitle = {Proceedings of the 2021 Research on Offensive and Defensive Techniques in the Context of Man At The End (MATE) Attacks},
pages = {59–69},
numpages = {11},
keywords = {reverse engineering, binary analysis},
location = {Virtual Event, Republic of Korea},
series = {Checkmate '21}
}



@article{DBLP:journals/corr/abs-2106-01540,
  author    = {Xuezhe Ma and
               Xiang Kong and
               Sinong Wang and
               Chunting Zhou and
               Jonathan May and
               Hao Ma and
               Luke Zettlemoyer},
  title     = {Luna: Linear Unified Nested Attention},
  journal   = {CoRR},
  volume    = {abs/2106.01540},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.01540},
  eprinttype = {arXiv},
  eprint    = {2106.01540},
  timestamp = {Thu, 10 Jun 2021 16:34:18 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2106-01540.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{spangher-etal-2021-multitask,
    title = "Multitask Semi-Supervised Learning for Class-Imbalanced Discourse Classification",
    author = "Spangher, Alexander  and
      May, Jonathan  and
      Shiang, Sz-Rung  and
      Deng, Lingjia",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.40",
    pages = "498--517",
    abstract = "As labeling schemas evolve over time, small differences can render datasets following older schemas unusable. This prevents researchers from building on top of previous annotation work and results in the existence, in discourse learning in particular, of many small class-imbalanced datasets. In this work, we show that a multitask learning approach can combine discourse datasets from similar and diverse domains to improve discourse classification. We show an improvement of 4.9{\%} Micro F1-score over current state-of-the-art benchmarks on the \textit{NewsDiscourse} dataset, one of the largest discourse datasets recently published, due in part to label correlations across tasks, which improve performance for underrepresented classes. We also offer an extensive review of additional techniques proposed to address resource-poor problems in NLP, and show that none of these approaches can improve classification accuracy in our setting.",
}

@inproceedings{zhang-etal-2021-salience,
    title = "Salience-Aware Event Chain Modeling for Narrative Understanding",
    author = "Zhang, Xiyang  and
      Chen, Muhao  and
      May, Jonathan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.107",
    pages = "1418--1428",
    abstract = "Storytelling, whether via fables, news reports, documentaries, or memoirs, can be thought of as the communication of interesting and related events that, taken together, form a concrete process. It is desirable to extract the event chains that represent such processes. However, this extraction remains a challenging problem. We posit that this is due to the nature of the texts from which chains are discovered. Natural language text interleaves a narrative of concrete, salient events with background information, contextualization, opinion, and other elements that are important for a variety of necessary discourse and pragmatics acts but are not part of the principal chain of events being communicated. We introduce methods for extracting this principal chain from natural language text, by filtering away non-salient events and supportive sentences. We demonstrate the effectiveness of our methods at isolating critical event chains by comparing their effect on downstream tasks. We show that by pre-training large language models on our extracted chains, we obtain improvements in two tasks that benefit from a clear understanding of event chains: narrative prediction and event-based temporal question answering. The demonstrated improvements and ablative studies confirm that our extraction method isolates critical event chains.",
}

@inproceedings{gheini-etal-2021-cross,
    title = "Cross-Attention is All You Need: {A}dapting Pretrained {T}ransformers for Machine Translation",
    author = "Gheini, Mozhdeh  and
      Ren, Xiang  and
      May, Jonathan",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.132",
    pages = "1754--1765",
    abstract = "We study the power of cross-attention in the Transformer architecture within the context of transfer learning for machine translation, and extend the findings of studies into cross-attention when training from scratch. We conduct a series of experiments through fine-tuning a translation model on data where either the source or target language has changed. These experiments reveal that fine-tuning only the cross-attention parameters is nearly as effective as fine-tuning all parameters (i.e., the entire translation model). We provide insights into why this is the case and observe that limiting fine-tuning in this manner yields cross-lingually aligned embeddings. The implications of this finding for researchers and practitioners include a mitigation of catastrophic forgetting, the potential for zero-shot translation, and the ability to extend machine translation models to several new language pairs with reduced parameter storage overhead.",
}


@inproceedings{yin-etal-2021-summary,
    title = "Summary-Oriented Question Generation for Informational Queries",
    author = "Yin, Xusen  and
      Zhou, Li  and
      Small, Kevin  and
      May, Jonathan",
    booktitle = "Proceedings of the 1st Workshop on Document-grounded Dialogue and Conversational Question Answering (DialDoc 2021)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.dialdoc-1.11",
    pages = "81--97",
    abstract = "Users frequently ask simple factoid questions for question answering (QA) systems, attenuating the impact of myriad recent works that support more complex questions. Prompting users with automatically generated suggested questions (SQs) can improve user understanding of QA system capabilities and thus facilitate more effective use. We aim to produce self-explanatory questions that focus on main document topics and are answerable with variable length passages as appropriate. We satisfy these requirements by using a BERT-based Pointer-Generator Network trained on the Natural Questions (NQ) dataset. Our model shows SOTA performance of SQ generation on the NQ dataset (20.1 BLEU-4). We further apply our model on out-of-domain news articles, evaluating with a QA system due to the lack of gold questions and demonstrate that our model produces better SQs for news articles {--} with further confirmation via a human evaluation.",
}



@misc{gheini2021strengths,
      title={On the Strengths of Cross-Attention in Pretrained Transformers for Machine Translation}, 
      author={Mozhdeh Gheini and Xiang Ren and Jonathan May},
      year={2021},
      eprint={2104.08771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{DBLP:journals/corr/abs-1909-06516,
  author    = {Mozhdeh Gheini and
               Jonathan May},
  title     = {A Universal Parent Model for Low-Resource Neural Machine Translation
               Transfer},
  journal   = {CoRR},
  volume    = {abs/1909.06516},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.06516},
  archivePrefix = {arXiv},
  eprint    = {1909.06516},
  timestamp = {Mon, 23 Sep 2019 18:07:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-06516.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{gowda-etal-2021-many,
    title = "Many-to-{E}nglish Machine Translation Tools, Data, and Pretrained Models",
    author = "Gowda, Thamme  and
      Zhang, Zhao  and
      Mattmann, Chris  and
      May, Jonathan",
    booktitle = "Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-demo.37",
    pages = "306--316",
    abstract = "While there are more than 7000 languages in the world, most translation research efforts have targeted a few high resource languages. Commercial translation systems support only one hundred languages or fewer, and do not make these models available for transfer to low resource languages. In this work, we present useful tools for machine translation research: MTData, NLCodec and RTG. We demonstrate their usefulness by creating a multilingual neural machine translation model capable of translating from 500 source languages to English. We make this multilingual model readily downloadable and usable as a service, or as a parent model for transfer-learning to even lower-resource languages.",
}


@inproceedings{hambardzumyan-etal-2021-warp,
    title = "{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming",
    author = "Hambardzumyan, Karen  and
      Khachatrian, Hrant  and
      May, Jonathan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.381",
    pages = "4921--4933",
    abstract = "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.",
}

@inproceedings{aldarrab-may-2021-sequence,
    title = "Can Sequence-to-Sequence Models Crack Substitution Ciphers?",
    author = "Aldarrab, Nada  and
      May, Jonathan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.561",
    pages = "7226--7235",
    abstract = "Decipherment of historical ciphers is a challenging problem. The language of the target plaintext might be unknown, and ciphertext can have a lot of noise. State-of-the-art decipherment methods use beam search and a neural language model to score candidate plaintext hypotheses for a given cipher, assuming the plaintext language is known. We propose an end-to-end multilingual model for solving simple substitution ciphers. We test our model on synthetic and real historical ciphers and show that our proposed method can decipher text without explicit language identification while still being robust to noise.",
}

@inproceedings{mhamdi-etal-2021-x,
    title = "{X}-{METRA}-{ADA}: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering",
    author = "M{'}hamdi, Meryem  and
      Kim, Doo Soon  and
      Dernoncourt, Franck  and
      Bui, Trung  and
      Ren, Xiang  and
      May, Jonathan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.283",
    pages = "3617--3632",
    abstract = "Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation.",
}


@inproceedings{chawla-etal-2021-casino,
    title = "{C}a{S}i{N}o: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems",
    author = "Chawla, Kushal  and
      Ramirez, Jaysa  and
      Clever, Rene  and
      Lucas, Gale  and
      May, Jonathan  and
      Gratch, Jonathan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.254",
    pages = "3167--3185",
    abstract = "Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https://github.com/kushalchawla/CaSiNo",
}

@inproceedings{gowda-etal-2021-macro,
    title = "Macro-Average: Rare Types Are Important Too",
    author = "Gowda, Thamme  and
      You, Weiqiu  and
      Lignos, Constantine  and
      May, Jonathan",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.naacl-main.90",
    pages = "1138--1157",
    abstract = "While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, MacroF1, and study its applicability to MT evaluation. We find that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MacroF1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods{'} outputs.",
}


@InBook{mythodologies-2019,
  author =    {Joseph A. Dane and Jonathan May},
  title =        {Begging The Question: Critical Reasoning in Chaucer Studies, Book History, and Humanistic Inquiry},
  chapter =      {III.3: Evidence and Artificial Intelligence},
  publisher =    {Marymount Institute Press},
  year =         2019,
  number =    {II},
  series =    {Mythodologies},
  pages =     {295--208}}


@misc{yin2020question,
      title={Question Generation for Supporting Informational Query Intents}, 
      author={Xusen Yin and Jonathan May and Li Zhou and Kevin Small},
      year={2020},
      eprint={2010.09692},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{aldarrab2020sequencetosequence,
      title={Can Sequence-to-Sequence Models Crack Substitution Ciphers?}, 
      author={Nada Aldarrab and Jonathan May},
      year={2020},
      eprint={2012.15229},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{hambardzumyan2021warp,
      title={WARP: Word-level Adversarial ReProgramming}, 
      author={Karen Hambardzumyan and Hrant Khachatrian and Jonathan May},
      year={2021},
      eprint={2101.00121},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{spangher2021multitask,
      title={Multitask Learning for Class-Imbalanced Discourse Classification}, 
      author={Alexander Spangher and Jonathan May and Sz-rung Shiang and Lingjia Deng},
      year={2021},
      eprint={2101.00389},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@proceedings{semeval-2020-semantic,
    title = "Proceedings of the Fourteenth Workshop on Semantic Evaluation",
    editor = "Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Palmer, Alexis  and
      Schneider, Nathan  and
      May, Jonathan  and
      Shutova, Ekaterina",
    month = dec,
    year = "2020",
    address = "Barcelona (online)",
    publisher = "International Committee for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.semeval-1.0",
}

@proceedings{nlp-covid19-2020-nlp,
    title = "Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020",
    editor = "Verspoor, Karin  and
      Cohen, Kevin Bretonnel  and
      Dredze, Mark  and
      Ferrara, Emilio  and
      May, Jonathan  and
      Munro, Robert  and
      Paris, Cecile  and
      Wallace, Byron",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.nlpcovid19-acl.0",
}

@inproceedings{spangher-etal-2020-enabling,
    title = "Enabling Low-Resource Transfer Learning across {COVID-19} Corpora by Combining Event-Extraction and Co-Training",
    author = "Spangher, Alexander  and
      Peng, Nanyun  and
      May, Jonathan  and
      Ferrara, Emilio",
    booktitle = "Proceedings of the 1st Workshop on {NLP} for {COVID-19} at {ACL} 2020",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.nlpcovid19-acl.4",
}

@inproceedings{yin-etal-2020-learning,
    title = "Learning to Generalize for Sequential Decision Making",
    author = "Yin, Xusen  and
      Weischedel, Ralph  and
      May, Jonathan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.273",
    pages = "3046--3063",
    abstract = "We consider problems of making sequences of decisions to accomplish tasks, interacting via the medium of language. These problems are often tackled with reinforcement learning approaches. We find that these models do not generalize well when applied to novel task domains. However, the large amount of computation necessary to adequately train and explore the search space of sequential decision making, under a reinforcement learning paradigm, precludes the inclusion of large contextualized language models, which might otherwise enable the desired generalization ability. We introduce a teacher-student imitation learning methodology and a means of converting a reinforcement learning model into a natural language understanding model. Together, these methodologies enable the introduction of contextualized language models into the sequential decision making problem space. We show that models can learn faster and generalize more, leveraging both the imitation learning and the reformulation. Our models exceed teacher performance on various held-out decision problems, by up to 7{\%} on in-domain problems and 24{\%} on out-of-domain problems.",
}

@inproceedings{gowda-may-2020-finding,
    title = "Finding the Optimal Vocabulary Size for Neural Machine Translation",
    author = "Gowda, Thamme  and
      May, Jonathan",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.findings-emnlp.352",
    pages = "3955--3964",
    abstract = "We cast neural machine translation (NMT) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore its effect on NMT. We analyze the effect of various vocabulary sizes on NMT performance on multiple languages with many data sizes, and reveal an explanation for why certain vocabulary sizes are better than others.",
}

@inproceedings{li-etal-2020-connecting,
    title = "Connecting the Dots: Event Graph Schema Induction with Path Language Modeling",
    author = "Li, Manling  and
      Zeng, Qi  and
      Lin, Ying  and
      Cho, Kyunghyun  and
      Ji, Heng  and
      May, Jonathan  and
      Chambers, Nathanael  and
      Voss, Clare",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.50",
    pages = "684--695",
    abstract = "Event schemas can guide our understanding and ability to make predictions with respect to what might happen next. We propose a new Event Graph Schema, where two event types are connected through multiple paths involving entities that fill important roles in a coherent story. We then introduce Path Language Model, an auto-regressive language model trained on event-event paths, and select salient and coherent paths to probabilistically construct these graph schemas. We design two evaluation metrics, instance coverage and instance coherence, to evaluate the quality of graph schema induction, by checking when coherent event instances are covered by the schema graph. Intrinsic evaluations show that our approach is highly effective at inducing salient and coherent schemas. Extrinsic evaluations show the induced schema repository provides significant improvement to downstream end-to-end Information Extraction over a state-of-the-art joint neural extraction model, when used as additional global features to unfold instance graphs.",
}

@inproceedings{bisk-etal-2020-experience,
    title = "Experience Grounds Language",
    author = "Bisk, Yonatan  and
      Holtzman, Ari  and
      Thomason, Jesse  and
      Andreas, Jacob  and
      Bengio, Yoshua  and
      Chai, Joyce  and
      Lapata, Mirella  and
      Lazaridou, Angeliki  and
      May, Jonathan  and
      Nisnevich, Aleksandr  and
      Pinto, Nicolas  and
      Turian, Joseph",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.703",
    pages = "8718--8735",
    abstract = "Language understanding research is held back by a failure to relate language to the physical world it describes and to the social interactions it facilitates. Despite the incredible effectiveness of language processing models to tackle tasks after being trained on text alone, successful linguistic communication relies on a shared experience of the world. It is this shared experience that makes utterances meaningful. Natural language processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large, text-only corpora requires the parallel tradition of research on the broader physical and social context of language to address the deeper questions of communication.",
}

@inproceedings{lu-etal-2020-cross,
    title = "Cross-lingual Structure Transfer for Zero-resource Event Extraction",
    author = "Lu, Di  and
      Subburathinam, Ananya  and
      Ji, Heng  and
      May, Jonathan  and
      Chang, Shih-Fu  and
      Sil, Avi  and
      Voss, Clare",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.243",
    pages = "1976--1981",
    abstract = "Most of the current cross-lingual transfer learning methods for Information Extraction (IE) have been only applied to name tagging. To tackle more complex tasks such as event extraction we need to transfer graph structures (event trigger linked to multiple arguments with various roles) across languages. We develop a novel share-and-transfer framework to reach this goal with three steps: (1) Convert each sentence in any language to language-universal graph structures; in this paper we explore two approaches based on universal dependency parses and complete graphs, respectively. (2) Represent each node in the graph structure with a cross-lingual word embedding so that all sentences in multiple languages can be represented with one shared semantic space. (3) Using this common semantic space, train event extractors from English training data and apply them to languages that do not have any event annotations. Experimental results on three languages (Spanish, Russian and Ukrainian) without any annotations show this framework achieves comparable performance to a state-of-the-art supervised model trained from more than 1,500 manually annotated event mentions.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{cho-may-2020-grounding,
    title = "Grounding Conversations with Improvised Dialogues",
    author = "Cho, Hyundong  and
      May, Jonathan",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.218",
    doi = "10.18653/v1/2020.acl-main.218",
    pages = "2398--2413",
    abstract = "Effective dialogue involves grounding, the process of establishing mutual knowledge that is essential for communication between people. Modern dialogue systems are not explicitly trained to build common ground, and therefore overlook this important aspect of communication. Improvisational theater (improv) intrinsically contains a high proportion of dialogue focused on building common ground, and makes use of the yes-and principle, a strong grounding speech act, to establish coherence and an actionable objective reality. We collect a corpus of more than 26,000 yes-and turns, transcribing them from improv dialogues and extracting them from larger, but more sparsely populated movie script dialogue corpora, via a bootstrapped classifier. We fine-tune chit-chat dialogue systems with our corpus to encourage more grounded, relevant conversation and confirm these findings with human evaluations.",
}


@InProceedings{spangher-2020-quote,
  author =       {Alexander Spangher, Nanyun Peng, Jonathan May, and Emilio Ferrara},
  title =        {“Don’t quote me on that”: Finding Mixtures of Sources in
News Articles},
  booktitle = {Proc. Computation+Journalism Symposium},
  year =      2020,
  month =     {March},
  address =   {Online}}

@inproceedings{lu-etal-2020-cross,
    title = "Cross-lingual Structure Transfer for Zero-resource Event Extraction",
    author = "Lu, Di  and
      Subburathinam, Ananya  and
      Ji, Heng  and
      May, Jonathan  and
      Chang, Shih-Fu  and
      Sil, Avi  and
      Voss, Clare",
    booktitle = "Proceedings of The 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://www.aclweb.org/anthology/2020.lrec-1.243",
    pages = "1976--1981",
    abstract = "Most of the current cross-lingual transfer learning methods for Information Extraction (IE) have been only applied to name tagging. To tackle more complex tasks such as event extraction we need to transfer graph structures (event trigger linked to multiple arguments with various roles) across languages. We develop a novel share-and-transfer framework to reach this goal with three steps: (1) Convert each sentence in any language to language-universal graph structures; in this paper we explore two approaches based on universal dependency parses and complete graphs, respectively. (2) Represent each node in the graph structure with a cross-lingual word embedding so that all sentences in multiple languages can be represented with one shared semantic space. (3) Using this common semantic space, train event extractors from English training data and apply them to languages that do not have any event annotations. Experimental results on three languages (Spanish, Russian and Ukrainian) without any annotations show this framework achieves comparable performance to a state-of-the-art supervised model trained from more than 1,500 manually annotated event mentions.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{pan-etal-2019-cross,
    title = "Cross-lingual Joint Entity and Word Embedding to Improve Entity Linking and Parallel Sentence Mining",
    author = "Pan, Xiaoman  and
      Gowda, Thamme  and
      Ji, Heng  and
      May, Jonathan  and
      Miller, Scott",
    booktitle = "Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-6107",
    doi = "10.18653/v1/D19-6107",
    pages = "56--66",
    abstract = "Entities, which refer to distinct objects in the real world, can be viewed as language universals and used as effective signals to generate less ambiguous semantic representations and align multiple languages. We propose a novel method, CLEW, to generate cross-lingual data that is a mix of entities and contextual words based on Wikipedia. We replace each anchor link in the source language with its corresponding entity title in the target language if it exists, or in the source language otherwise. A cross-lingual joint entity and word embedding learned from this kind of data not only can disambiguate linkable entities but can also effectively represent unlinkable entities. Because this multilingual common space directly relates the semantics of contextual words in the source language to that of entities in the target language, we leverage it for unsupervised cross-lingual entity linking. Experimental results show that CLEW significantly advances the state-of-the-art: up to 3.1{\%} absolute F-score gain for unsupervised cross-lingual entity linking. Moreover, it provides reliable alignment on both the word/entity level and the sentence level, and thus we use it to mine parallel sentences for all (302, 2) language pairs in Wikipedia.",
}

@inproceedings{mhamdi-etal-2019-contextualized,
    title = "Contextualized Cross-Lingual Event Trigger Extraction with Minimal Resources",
    author = "M{'}hamdi, Meryem  and
      Freedman, Marjorie  and
      May, Jonathan",
    booktitle = "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/K19-1061",
    doi = "10.18653/v1/K19-1061",
    pages = "656--665",
    abstract = "Event trigger extraction is an information extraction task of practical utility, yet it is challenging due to the difficulty of disambiguating word sense meaning. Previous approaches rely extensively on hand-crafted language-specific features and are applied mainly to English for which annotated datasets and Natural Language Processing (NLP) tools are available. However, the availability of such resources varies from one language to another. Recently, contextualized Bidirectional Encoder Representations from Transformers (BERT) models have established state-of-the-art performance for a variety of NLP tasks. However, there has not been much effort in exploring language transfer using BERT for event extraction. In this work, we treat event trigger extraction as a sequence tagging problem and propose a cross-lingual framework for training it without any hand-crafted features. We experiment with different flavors of transfer learning from high-resourced to low-resourced languages and compare the performance of different multilingual embeddings for event trigger extraction. Our results show that training in a multilingual setting outperforms language-specific models for both English and Chinese. Our work is the first to experiment with two event architecture variants in a cross-lingual setting, to show the effectiveness of contextualized embeddings obtained using BERT, and to explore and analyze its performance on Arabic.",
}

@inproceedings{mullenbach-etal-2019-nuclear,
    title = "Do Nuclear Submarines Have Nuclear Captains? A Challenge Dataset for Commonsense Reasoning over Adjectives and Objects",
    author = "Mullenbach, James  and
      Gordon, Jonathan  and
      Peng, Nanyun  and
      May, Jonathan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1625",
    doi = "10.18653/v1/D19-1625",
    pages = "6051--6057",
    abstract = "How do adjectives project from a noun to its parts? If a motorcycle is red, are its wheels red? Is a nuclear submarine{'}s captain nuclear? These questions are easy for humans to judge using our commonsense understanding of the world, but are difficult for computers. To attack this challenge, we crowdsource a set of human judgments that answer the English-language question {``}Given a whole described by an adjective, does the adjective also describe a given part?{''} We build strong baselines for this task with a classification approach. Our findings indicate that, despite the recent successes of large language models on tasks aimed to assess commonsense knowledge, these models do not greatly outperform simple word-level models based on pre-trained word embeddings. This provides evidence that the amount of commonsense knowledge encoded in these language models does not extend far beyond that already baked into the word embeddings. Our dataset will serve as a useful testbed for future research in commonsense reasoning, especially as it relates to adjectives and objects",
}

@inproceedings{huang-etal-2019-matters,
    title = "What Matters for Neural Cross-Lingual Named Entity Recognition: An Empirical Analysis",
    author = "Huang, Xiaolei  and
      May, Jonathan  and
      Peng, Nanyun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1672",
    doi = "10.18653/v1/D19-1672",
    pages = "6394--6400",
    abstract = "Building named entity recognition (NER) models for languages that do not have much training data is a challenging task. While recent work has shown promising results on cross-lingual transfer from high-resource languages, it is unclear what knowledge is transferred. In this paper, we first propose a simple and efficient neural architecture for cross-lingual NER. Experiments show that our model achieves competitive performance with the state-of-the-art. We further explore how transfer learning works for cross-lingual NER on two transferable factors: sequential order and multilingual embedding. Our results shed light on future research for improving cross-lingual NER.",
}

@inproceedings{subburathinam-etal-2019-cross,
    title = "Cross-lingual Structure Transfer for Relation and Event Extraction",
    author = "Subburathinam, Ananya  and
      Lu, Di  and
      Ji, Heng  and
      May, Jonathan  and
      Chang, Shih-Fu  and
      Sil, Avirup  and
      Voss, Clare",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1030",
    doi = "10.18653/v1/D19-1030",
    pages = "313--325",
    abstract = "The identification of complex semantic structures such as events and entity relations, already a challenging Information Extraction task, is doubly difficult from sources written in under-resourced and under-annotated languages. We investigate the suitability of cross-lingual structure transfer techniques for these tasks. We exploit relation- and event-relevant language-universal features, leveraging both symbolic (including part-of-speech and dependency path) and distributional (including type representation and contextualized representation) information. By representing all entity mentions, event triggers, and contexts into this complex and structured multilingual common space, using graph convolutional networks, we can train a relation or event extractor from source language annotations and apply it to the target language. Extensive experiments on cross-lingual relation and event transfer among English, Chinese, and Arabic demonstrate that our approach achieves performance comparable to state-of-the-art supervised models trained on up to 3,000 manually annotated mentions: up to 62.6{\%} F-score for Relation Extraction, and 63.1{\%} F-score for Event Argument Role Labeling. The event argument role labeling model transferred from English to Chinese achieves similar performance as the model trained from Chinese. We thus find that language-universal symbolic and distributional representations are complementary for cross-lingual structure transfer.",
}

@inproceedings{Yin2019ComprehensibleCT,
  title={Comprehensible Context-driven Text Game Playing},
  author={Xusen Yin and Jonathan May},
  booktitle={Proc. 2019 IEEE Conference on Games (CoG)},
  year={2019},
  month=August,
  pages={1-8},
}

@inproceedings{boschee-etal-2019-saral,
    title = "{SARAL}: A Low-Resource Cross-Lingual Domain-Focused Information Retrieval System for Effective Rapid Document Triage",
    author = "Boschee, Elizabeth  and
      Barry, Joel  and
      Billa, Jayadev  and
      Freedman, Marjorie  and
      Gowda, Thamme  and
      Lignos, Constantine  and
      Palen-Michel, Chester  and
      Pust, Michael  and
      Khonglah, Banriskhem Kayang  and
      Madikeri, Srikanth  and
      May, Jonathan  and
      Miller, Scott",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-3004",
    doi = "10.18653/v1/P19-3004",
    pages = "19--24",
    abstract = "With the increasing democratization of electronic media, vast information resources are available in less-frequently-taught languages such as Swahili or Somali. That information, which may be crucially important and not available elsewhere, can be difficult for monolingual English speakers to effectively access. In this paper we present an end-to-end cross-lingual information retrieval (CLIR) and summarization system for low-resource languages that 1) enables English speakers to search foreign language repositories of text and audio using English queries, 2) summarizes the retrieved documents in English with respect to a particular information need, and 3) provides complete transcriptions and translations as needed. The SARAL system achieved the top end-to-end performance in the most recent IARPA MATERIAL CLIR+summarization evaluations. Our demonstration system provides end-to-end open query retrieval and summarization capability, and presents the original source text or audio, speech transcription, and machine translation, for two low resource languages.",
}

@inproceedings{pourdamghani-etal-2019-translating,
    title = "Translating Translationese: A Two-Step Approach to Unsupervised Machine Translation",
    author = "Pourdamghani, Nima  and
      Aldarrab, Nada  and
      Ghazvininejad, Marjan  and
      Knight, Kevin  and
      May, Jonathan",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1293",
    doi = "10.18653/v1/P19-1293",
    pages = "3057--3062",
    abstract = "Given a rough, word-by-word gloss of a source language sentence, target language natives can uncover the latent, fully-fluent rendering of the translation. In this work we explore this intuition by breaking translation into a two step process: generating a rough gloss by means of a dictionary and then {`}translating{'} the resulting pseudo-translation, or {`}Translationese{'} into a fully fluent translation. We build our Translationese decoder once from a mish-mash of parallel data that has the target language in common and then can build dictionaries on demand using unsupervised techniques, resulting in rapidly generated unsupervised neural MT systems for many source languages. We apply this process to 14 test languages, obtaining better or comparable translation results on high-resource languages than previously published unsupervised MT studies, and obtaining good quality results for low-resource languages that have never been used in an unsupervised MT scenario.",
}

@proceedings{semeval-2019-international,
    title = "Proceedings of the 13th International Workshop on Semantic Evaluation",
    editor = "May, Jonathan  and
      Shutova, Ekaterina  and
      Herbelot, Aurelie  and
      Zhu, Xiaodan  and
      Apidianaki, Marianna  and
      Mohammad, Saif M.",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S19-2000",
}

@InProceedings{huang-ji-may:2019:N19-1,
  author    = {Huang, Lifu  and  Ji, Heng  and  May, Jonathan},
  title     = {Cross-lingual Multi-Level Adversarial Transfer to Enhance Low-Resource Name Tagging},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = {June},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  pages     = {3823--3833},
  abstract  = {We focus on improving name tagging for low-resource languages using annotations from related languages. Previous studies either directly project annotations from a source language to a target language using cross-lingual representations or use a shared encoder in a multitask network to transfer knowledge. These approaches inevitably introduce noise to the target language annotation due to mismatched source-target sentence structures. To effectively transfer the resources, we develop a new neural architecture that leverages multi-level adversarial transfer: (1) word-level adversarial training, which projects source language words into the same semantic space as those of the target language without using any parallel corpora or bilingual gazetteers, and (2) sentence-level adversarial training, which yields language-agnostic sequential features. Our neural architecture outperforms previous approaches on CoNLL data sets. Moreover, on 10 low-resource languages, our approach achieves up to 16\% absolute F-score gain over all high-performing baselines on cross-lingual transfer without using any target-language resources.},
  url       = {http://www.aclweb.org/anthology/N19-1383}
}

@InProceedings{cardenas-EtAl:2019:N19-1,
  author    = {Cardenas, Ronald  and  Lin, Ying  and  Ji, Heng  and  May, Jonathan},
  title     = {A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  month     = {June},
  year      = {2019},
  address   = {Minneapolis, Minnesota},
  publisher = {Association for Computational Linguistics},
  pages     = {2428--2439},
  abstract  = {Unsupervised part of speech (POS) tagging is often framed as a clustering problem, but practical taggers need to ground their clusters as well. Grounding generally requires reference labeled data, a luxury a low-resource language might not have. In this work, we describe an approach for low-resource unsupervised POS tagging that yields fully grounded output and requires no labeled training data. We find the classic method of Brown et al. (1992) clusters well in our use case and employ a decipherment-based approach to grounding. This approach presumes a sequence of cluster IDs is a `ciphertext' and seeks a POS tag-to-cluster ID mapping that will reveal the POS sequence. We show intrinsically that, despite the difficulty of the task, we obtain reasonable performance across a variety of languages. We also show extrinsically that incorporating our POS tagger into a name tagger leads to state-of-the-art tagging performance in Sinhalese and Kinyarwanda, two languages with nearly no labeled POS data available. We further demonstrate our tagger's utility by incorporating it into a true `zero-resource' variant of the MALOPA (Ammar et al., 2016) dependency parser model that removes the current reliance on multilingual resources and gold POS tags for new languages. Experiments show that including our tagger makes up much of the accuracy lost when gold POS tags are unavailable.},
  url       = {http://www.aclweb.org/anthology/N19-1252}
}

@InProceedings{hermjakob-EtAl:2018:Demos,
  author    = {Hermjakob, Ulf  and  May, Jonathan  and  Pust, Michael  and  Knight, Kevin},
  title     = {Translating a Language You Don't Know In the Chinese Room},
  booktitle = {Proceedings of ACL 2018, System Demonstrations},
  month     = {July},
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  pages     = {62--67},
  abstract  = {In a corruption of John Searle's famous AI thought experiment, the Chinese Room (Searle, 1980), we twist its original intent by enabling humans to translate text, e.g. from Uyghur to English, even if they don't have any prior knowledge of the source language. Our enabling tool, which we call the Chinese Room, is equipped with the same resources made available to a machine translation engine. We find that our superior language model and world knowledge allows us to create perfectly fluent and nearly adequate translations, with human expertise required only for the target language. The Chinese Room tool can be used to rapidly create small corpora of parallel data when bilingual translators are not readily available, in particular for low-resource languages.},
  url       = {http://www.aclweb.org/anthology/P18-4011}
}


@InProceedings{hermjakob-may-knight:2018:Demos,
  author    = {Hermjakob, Ulf  and  May, Jonathan  and  Knight, Kevin},
  title     = {Out-of-the-box Universal Romanization Tool uroman},
  booktitle = {Proceedings of ACL 2018, System Demonstrations},
  month     = {July},
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics},
  pages     = {13--18},
  abstract  = {We present uroman, a tool for converting text in myriads of languages and scripts such as Chinese, Arabic and Cyrillic into a common Latin-script representation. The tool relies on Unicode data and other tables, and handles nearly all character sets, including some that are quite obscure such as Tibetan and Tifinagh. uroman converts digital numbers in various scripts to Western Arabic numerals. Romanization enables the application of string-similarity metrics to texts from different scripts without the need and complexity of an intermediate phonetic representation. The tool is freely and publicly available as a Perl script suitable for inclusion in data processing pipelines and as an interactive demo web page.},
  url       = {http://www.aclweb.org/anthology/P18-4003}
}


@InProceedings{peng-EtAl:2018:W18-15,
  author    = {Peng, Nanyun  and  Ghazvininejad, Marjan  and  May, Jonathan  and  Knight, Kevin},
  title     = {Towards Controllable Story Generation},
  booktitle = {Proceedings of the First Workshop on Storytelling},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {43--49},
  abstract  = {We present a general framework of analyzing existing story corpora to generate controllable and creative new stories. The proposed framework needs little manual annotation to achieve controllable story generation. It creates a new interface for humans to interact with computers to generate personalized stories. We apply the framework to build recurrent neural network (RNN)-based generation models to control story ending valence and storyline. Experiments show that our methods successfully achieve the control and enhance the coherence of stories through introducing storylines. with additional control factors, the generation model gets lower perplexity, and yields more coherent stories that are faithful to the control factors according to human evaluation.},
  url       = {http://www.aclweb.org/anthology/W18-1505}
}


@Book{S18-1:2018,
  editor    = {Marianna Apidianaki  and  Saif M. Mohammad  and  Jonathan May  and  Ekaterina Shutova  and  Steven Bethard  and  Marine Carpuat},
  title     = {Proceedings of The 12th International Workshop on Semantic Evaluation},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  url       = {http://www.aclweb.org/anthology/S18-1}
}


@InProceedings{zhang-EtAl:2018:N18-5,
  author    = {Zhang, Boliang  and  Lin, Ying  and  Pan, Xiaoman  and  Lu, Di  and  May, Jonathan  and  Knight, Kevin  and  Ji, Heng},
  title     = {ELISA-EDL: A Cross-lingual Entity Extraction, Linking and Localization System},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Demonstrations},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {41--45},
  abstract  = {We demonstrate ELISA-EDL, a state-of-the-art re-trainable system to extract entity mentions from low-resource languages, link them to external English knowledge bases, and visualize locations related to disaster topics on a world heatmap. We make all of our data sets, resources and system training and testing APIs publicly available for research purpose.},
  url       = {http://www.aclweb.org/anthology/N18-5009}
}

@InProceedings{chen-EtAl:2018:N18-14,
  author    = {Chen, Yining  and  Gilroy, Sorcha  and  Maletti, Andreas  and  May, Jonathan  and  Knight, Kevin},
  title     = {Recurrent Neural Networks as Weighted Language Recognizers},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics},
  pages     = {2261--2271},
  abstract  = {We investigate the computational complexity of various problems for simple recurrent neural networks (RNNs) as formal models for recognizing weighted languages. We focus on the single-layer, ReLU-activation, rational-weight RNNs with softmax, which are commonly used in natural language processing applications. We show that most problems for such RNNs are undecidable, including consistency, equivalence, minimization, and the determination of the highest-weighted string. However, for consistent RNNs the last problem becomes decidable, although the solution length can surpass all computable bounds. If additionally the string is limited to polynomial length, the problem becomes NP-complete. In summary, this shows that approximations and heuristic algorithms are necessary in practical applications of those RNNs.},
  url       = {http://www.aclweb.org/anthology/N18-1205}
}

@Article{Hermjakob2018,
author="Hermjakob, Ulf
and Li, Qiang
and Marcu, Daniel
and May, Jonathan
and Mielke, Sebastian J.
and Pourdamghani, Nima
and Pust, Michael
and Shi, Xing
and Knight, Kevin
and Levinboim, Tomer
and Murray, Kenton
and Chiang, David
and Zhang, Boliang
and Pan, Xiaoman
and Lu, Di
and Lin, Ying
and Ji, Heng",
title="Incident-Driven Machine Translation and Name Tagging for Low-resource Languages",
journal="Machine Translation",
year="2018",
month="Jun",
day="01",
volume="32",
number="1",
pages="59--89",
abstract="We describe novel approaches to tackling the problem of natural language processing for low-resource languages. The approaches are embodied in systems for name tagging and machine translation (MT) that we constructed to participate in the NIST LoReHLT evaluation in 2016. Our methods include universal tools, rapid resource and knowledge acquisition, rapid language projection, and joint methods for MT and name tagging.",
issn="1573-0573",
doi="10.1007/s10590-017-9207-1",
url="https://doi.org/10.1007/s10590-017-9207-1"
}

@article{doi:10.1089/big.2017.0012,
author = { Huang Lifu  and  May Jonathan  and  Pan Xiaoman  and  Ji Heng  and  Ren Xiang  and  Han Jiawei  and  Zhao Lin  and  Hendler James A. },
title = {Liberal Entity Extraction: Rapid Construction of Fine-Grained Entity Typing Systems},
journal = {Big Data},
volume = {5},
number = {1},
pages = {19-31},
year = {2017},
doi = {10.1089/big.2017.0012},
note ={PMID: 28328252},
URL = {https://doi.org/10.1089/big.2017.0012},
eprint = {https://doi.org/10.1089/big.2017.0012},
abstract = { Abstract The ability of automatically recognizing and typing entities in natural language without prior knowledge (e.g., predefined entity types) is a major challenge in processing such data. Most existing entity typing systems are limited to certain domains, genres, and languages. In this article, we propose a novel unsupervised entity-typing framework by combining symbolic and distributional semantics. We start from learning three types of representations for each entity mention: general semantic representation, specific context representation, and knowledge representation based on knowledge bases. Then we develop a novel joint hierarchical clustering and linking algorithm to type all mentions using these representations. This framework does not rely on any annotated data, predefined typing schema, or handcrafted features; therefore, it can be quickly adapted to a new domain, genre, and/or language. Experiments on genres (news and discussion forum) show comparable performance with state-of-the-art supervised typing systems trained from a large amount of labeled data. Results on various languages (English, Chinese, Japanese, Hausa, and Yoruba) and domains (general and biomedical) demonstrate the portability of our framework. }
}

@InProceedings{may-priyadarshi:2017:SemEval,
  author    = {May, Jonathan  and  Priyadarshi, Jay},
  title     = {SemEval-2017 Task 9: Abstract Meaning Representation Parsing and Generation},
  booktitle = {Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)},
  month     = {August},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {536--545},
  abstract  = {In this report we summarize the results of the 2017 AMR SemEval shared task.
	The task consisted of two separate yet related subtasks. In the parsing
	subtask, participants were asked to produce Abstract Meaning Representation
	(AMR) (Banarescu et al., 2013) graphs for a set of English sentences in the
	biomedical domain. In the generation subtask, participants were asked to
	generate English sentences given AMR graphs in the news/forum domain. A total
	of five sites participated in the parsing subtask, and four participated in the
	generation subtask. 
	Along with a description of the task and the participants' systems, we show
	various score ablations and some sample outputs.},
  url       = {http://www.aclweb.org/anthology/S17-2090}
}

@InProceedings{pan-EtAl:2017:Long2,
  author    = {Pan, Xiaoman  and  Zhang, Boliang  and  May, Jonathan  and  Nothman, Joel  and  Knight, Kevin  and  Ji, Heng},
  title     = {Cross-lingual Name Tagging and Linking for 282 Languages},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {July},
  year      = {2017},
  address   = {Vancouver, Canada},
  publisher = {Association for Computational Linguistics},
  pages     = {1946--1958},
  abstract  = {The ambitious goal of this work is to develop a cross-lingual name tagging and
	linking framework for 282 languages that exist in Wikipedia. Given a document
	in any of these languages, our framework is able to identify name mentions,
	assign a coarse-grained or fine-grained type to each mention, and link it to an
	English Knowledge Base (KB) if it is linkable. We achieve this goal by
	performing a series of new KB mining methods: generating ``silver-standard''
	annotations by transferring annotations from English to other languages through
	cross-lingual links and KB properties, refining annotations through
	self-training and topic selection, deriving language-specific morphology
	features from anchor links, and mining word translation pairs from
	cross-lingual links. Both name tagging and linking results for 282 languages
	are promising on Wikipedia data and on-Wikipedia data.},
  url       = {http://aclweb.org/anthology/P17-1178}
}

@inproceedings{Papadopoulos2017,
  author={Pavlos Papadopoulos and Ruchir Travadi and Colin Vaz and Nikolaos Malandrakis and Ulf Hermjakob and Nima Pourdamghani and Michael Pust and Boliang Zhang and Xiaoman Pan and Di Lu and Ying Lin and Ondřej Glembek and Murali Karthick Baskar and Martin Karafiát and Lukáš Burget and Mark Hasegawa-Johnson and Heng Ji and Jonathan May and Kevin Knight and Shrikanth S. Narayanan},
  title={Team ELISA System for DARPA LORELEI Speech Evaluation 2016},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={2053--2057},
  doi={10.21437/Interspeech.2017-180},
  url={http://dx.doi.org/10.21437/Interspeech.2017-180}
}

@InProceedings{zoph-EtAl:2016:EMNLP2016,
  author    = {Zoph, Barret  and  Yuret, Deniz  and  May, Jonathan  and  Knight, Kevin},
  title     = {Transfer Learning for Low-Resource Neural Machine Translation},
  booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  month     = {November},
  year      = {2016},
  address   = {Austin, Texas},
  publisher = {Association for Computational Linguistics},
  pages     = {1568--1575},
  url       = {https://aclweb.org/anthology/D16-1163}
}

@InProceedings{may:2016:SemEval,
  author    = {May, Jonathan},
  title     = {SemEval-2016 Task 8: Meaning Representation Parsing},
  booktitle = {Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {1063--1073},
  url       = {http://www.aclweb.org/anthology/S16-1166}
}

@InProceedings{zoph-EtAl:2016:N16-1,
  author    = {Zoph, Barret  and  Vaswani, Ashish  and  May, Jonathan  and  Knight, Kevin},
  title     = {Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2016},
  address   = {San Diego, California},
  publisher = {Association for Computational Linguistics},
  pages     = {1217--1222},
  url       = {http://www.aclweb.org/anthology/N16-1145}
}

@InProceedings{CHOI16.581,
  author = {Eunsol Choi and Matic Horvat and Jonathan May and Kevin Knight and Daniel Marcu},
  title = {Extracting Structured Scholarly Information from the Machine Translation Literature},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016)},
  year = {2016},
  month = {may},
  date = {23-28},
  location = {Portorož, Slovenia},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Sara Goggi and Marko Grobelnik and Bente Maegaard and Joseph Mariani and Helene Mazo and Asuncion Moreno and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {978-2-9517408-9-1},
  language = {english}
 }

@InProceedings{pust-EtAl:2015:EMNLP,
  author    = {Pust, Michael  and  Hermjakob, Ulf  and  Knight, Kevin  and  Marcu, Daniel  and  May, Jonathan},
  title     = {Parsing English into Abstract Meaning Representation Using Syntax-Based Machine Translation},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  month     = {September},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  pages     = {1143--1154},
  url       = {http://aclweb.org/anthology/D15-1136}
}

@InProceedings{gordon-EtAl:2015:Metaphor2,
  author    = {Gordon, Jonathan  and  Hobbs, Jerry  and  May, Jonathan  and  Mohler, Michael  and  Morbini, Fabrizio  and  Rink, Bryan  and  Tomlinson, Marc  and  Wertheim, Suzanne},
  title     = {A Corpus of Rich Metaphor Annotation},
  booktitle = {Proceedings of the Third Workshop on Metaphor in NLP},
  month     = {June},
  year      = {2015},
  address   = {Denver, Colorado},
  publisher = {Association for Computational Linguistics},
  pages     = {56--66},
  url       = {http://www.aclweb.org/anthology/W15-1407}
}

@InProceedings{gordon-EtAl:2015:Metaphor1,
  author    = {Gordon, Jonathan  and  Hobbs, Jerry  and  May, Jonathan  and  Morbini, Fabrizio},
  title     = {High-Precision Abductive Mapping of Multilingual Metaphors},
  booktitle = {Proceedings of the Third Workshop on Metaphor in NLP},
  month     = {June},
  year      = {2015},
  address   = {Denver, Colorado},
  publisher = {Association for Computational Linguistics},
  pages     = {50--55},
  url       = {http://www.aclweb.org/anthology/W15-1406}
}

@InProceedings{may-benjira-echihabi:2014:AMTA,
  author    = {May, Jonathan  and  Benjira, Yassine and Echihabi, Abdessamad},
  title     = {An Arabizi-English Social Media Statistical Machine Translation System},
  booktitle = {Proceedings of the Eleventh Biennial Conference of the
Association for Machine Translation in the Americas},
  month     = {October},
  year      = {2014},
  address   = {Vancouver, Canada},
  publisher = {Association for Machine Translation in the Americas},
}

@inproceedings{10.5555/2540128.2540424,
author = {Barr\'{o}n-Cede\~{n}o, Alberto and M\`{a}rquez, Llu\'{\i}s and Henr\'{\i}quez, Q. Carlos A. and Formiga, Llu\'{\i}s and Romero, Enrique and May, Jonathan},
title = {Identifying useful human correction feedback from an on-line machine translation service},
year = {2013},
isbn = {9781577356332},
publisher = {AAAI Press},
abstract = {Post-editing feedback provided by users of on-line translation services offers an excellent opportunity for automatic improvement of statistical machine translation (SMT) systems. However, feedback provided by casual users is very noisy, and must be automatically filtered in order to identify the potentially useful cases. We present a study on automatic feedback filtering in a real weblog collected from Reverso.net. We extend and re-annotate a training corpus, define an extended set of simple features and approach the problem as a binary classification task, experimenting with linear and kernel-based classifiers and feature selection. Results on the feedback filtering task show a significant improvement over the majority class, but also a precision ceiling around 70-80\%. This reflects the inherent difficulty of the problem and indicates that shallow features cannot fully capture the semantic nature of the problem. Despite the modest results on the filtering task, the classifiers are proven effective in an application-based evaluation. The incorporation of a filtered set of feedback instances selected from a larger corpus significantly improves the performance of a phrase-based SMT system, according to a set of standard evaluation metrics.},
booktitle = {Proceedings of the Twenty-Third International Joint Conference on Artificial Intelligence},
pages = {2057–2063},
numpages = {7},
location = {Beijing, China},
series = {IJCAI '13}
}


@InProceedings{hopkins-may:2013:ACL2013,
  author    = {Hopkins, Mark  and  May, Jonathan},
  title     = {Models of Translation Competitions},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  month     = {August},
  year      = {2013},
  address   = {Sofia, Bulgaria},
  publisher = {Association for Computational Linguistics},
  pages     = {1416--1424},
  url       = {http://www.aclweb.org/anthology/P13-1139}
}

@InProceedings{PIGHIN12.337,
  author = {Daniele Pighin and Lluís Màrquez and Jonathan May},
  title = {An Analysis (and an Annotated Corpus) of User Responses to Machine Translation Output},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC'12)},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Uğur Doğan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
  language = {english}
}

@InProceedings{hopkins-may:2011:EMNLP,
  author    = {Hopkins, Mark  and  May, Jonathan},
  title     = {Tuning as Ranking},
  booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
  month     = {July},
  year      = {2011},
  address   = {Edinburgh, Scotland, UK.},
  publisher = {Association for Computational Linguistics},
  pages     = {1352--1362},
  url       = {http://www.aclweb.org/anthology/D11-1125}
}

@InProceedings{may-knight-vogler:2010:ACL,
  author    = {May, Jonathan  and  Knight, Kevin  and  Vogler, Heiko},
  title     = {Efficient Inference through Cascades of Weighted Tree Transducers},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  month     = {July},
  year      = {2010},
  address   = {Uppsala, Sweden},
  publisher = {Association for Computational Linguistics},
  pages     = {1058--1066},
  url       = {http://www.aclweb.org/anthology/P10-1108}
}

@Article{wangmayknightmarcu10:cl,
  author = 	 {Wei Wang and Jonathan May and Kevin Knight and Daniel Marcu},
  title = 	 {Re-structuring, Re-labeling, and Re-aligning for Syntax-Based Machine Translation },
  journal = 	 {Computational Linguistics},
  year = 	 2010,
  volume =	 36,
  number =	 2,
  pages =	 {247--277},
  month =	 {June}
}

% missing: "Determinization of Weighted Tree Automata using Factorizations", (M. Büchse, J. May, and H. Vogler), Proc. FSMNLP, 2009.
% missing: "Backward and Forward Bisimulation Minimization of Tree Automata", (J. Högberg, A. Maletti, and J. May), Theoretical Computer Science, volume 410, no. 37 (September, 2009).
% missing: "Applications of Weighted Automata in Natural Language Processing", (K. Knight and J. May), Handbook of Weighted Automata (M. Droste, W. Kuich, H. Vogler, eds.), 2009. 

@Article{graehlknightmay08:cl,
  author = 	 {Jonathan Graehl and Kevin Knight and Jonathan May},
  title = 	 {Training Tree Transducers},
  journal = 	 {Computational Linguistics},
  year = 	 2008,
  volume =	 34,
  number =	 3,
  pages =	 {391--427},
  month =	 {September}
}

@InProceedings{mayknight07:emnlp,
  author = 	 {Jonathan May and Kevin Knight},
  title = 	 {Syntactic Re-Alignment Models for Machine Translation},
  booktitle =	 {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning},
  pages =	 {360--368},
  year =	 2007,
  editor =	 {Jason Eisner and Taku Kudo},
  address =	 {Prague, Czech Republic},
  month =	 {June 28 -- June 30},
  publisher =	 {Association for Computational Linguistics}
}

@InProceedings{hogbergmalettimay07:ciaa,
  author = 	 {Johanna H\"{o}gberg and Andreas Maletti and Jonathan May},
  title = 	 {Backward and Forward Bisimulation Minimisation of Weighted Tree Automata},
  booktitle =	 {Proceedings of the 12th International Conference on Implementation and Application of Automata, CIAA 2007},
  pages =	 {109--121},
  year =	 2007,
  editor =	 {Jan Holub and Jan \v{Z}d\'{a}rek},
  volume =	 4783,
  series =	 {Lecture Notes in Computer Science},
  address =	 {Prague, Czech Republic},
  month =	 {July 16--18},
  publisher =	 {Springer-Verlag}
}

@InProceedings{hogbergmalettimay07:dlt,
  author = 	 {Johanna H\"{o}gberg and Andreas Maletti and Jonathan May},
  title = 	 {Bisimulation Minimisation for Weighted Tree Automata},
  booktitle =	 {Proceedings of the 11th International Conference on Developments in Language Theory, DLT 2007},
  pages =	 {229--240},
  year =	 2007,
  editor =	 {Tero Harju and Juhani Karhum\"{a}ki and Arto Lepist\"{o}},
  volume =	 4588,
  series =	 {Lecture Notes in Computer Science},
  address =	 {Turku, Finland},
  month =	 {July 3--6},
  publisher =	 {Springer-Verlag}
}

@InProceedings{mayknight06:ciaa,
  author = 	 {Jonathan May and Kevin Knight},
  title = 	 {Tiburon: A Weighted Tree Automata Toolkit},
  booktitle =	 {Proceedings of the 11th International Conference of Implementation and Application of Automata, CIAA 2006},
  pages =	 {102--113},
  year =	 2006,
  editor =	 {Oscar H. Ibarra and Hsu-Chun Yen},
  volume =	 4094,
  series =	 {Lecture Notes in Computer Science},
  address =	 {Taipei, Taiwan},
  month =	 {August},
  publisher =	 {Springer}
}

@InProceedings{mayknight06:naacl,
  author = 	 {Jonathan May and Kevin Knight},
  title = 	 {A Better N-Best List: Practical Determinization of Weighted Finite Tree Automata},
  booktitle =	 {Proceedings of the 2006 Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics},
  pages =	 {351--358},
  year =	 2006,
  editor =	 {Sanjeev Khudanpur and Brian Roark},
  volume =	 {Main Proceedings},
  address =	 {Brooklyn, New York},
  month =	 {June 5 -- June 7},
  publisher =	 {Association for Computational Linguistics}
}

@article{979873,
 author = {Jonathan May and Ada Brunstein and Prem Natarajan and Ralph Weischedel},
 title = {Surprise! What's in a Cebuano or Hindi Name?},
 journal = {ACM Transactions on Asian Language Information Processing (TALIP)},
 volume = {2},
 number = {3},
 year = {2003},
 issn = {1530-0226},
 pages = {169--180},
 doi = {http://doi.acm.org/10.1145/979872.979873},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 }

@inproceedings{DBLP:conf/ndqa/XuLMMW03,
  author    = {Jinxi Xu and
               Ana Licuanan and
               Jonathan May and
               Scott Miller and
               Ralph M. Weischedel},
  title     = {Answer Selection and Confidence Estimation.},
  booktitle = {New Directions in Question Answering},
  year      = {2003},
  pages     = {134-137},
  crossref  = {DBLP:conf/ndqa/2003},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@proceedings{DBLP:conf/ndqa/2003,
  editor    = {Mark T. Maybury},
  title     = {New Directions in Question Answering, Papers from 2003 AAAI
               Spring Symposium, Stanford University, Stanford, CA, USA},
  booktitle = {New Directions in Question Answering},
  publisher = {AAAI Press},
  year      = {2003},
  isbn      = {1-57735-184-3},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@inproceedings{DBLP:conf/trec/XuLMMW02,
  author    = {Jinxi Xu and
               Ana Licuanan and
               Jonathan May and
               Scott Miller and
               Ralph M. Weischedel},
  title     = {TREC 2002 QA at BBN: Answer Selection and Confidence Estimation.},
  booktitle = {TREC},
  year      = {2002},
  ee        = {http://trec.nist.gov/pubs/trec11/papers/bbn.xu.qa.pdf},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{DBLP:journals/corr/PustHKMM15,
  author    = {Michael Pust and
               Ulf Hermjakob and
               Kevin Knight and
               Daniel Marcu and
               Jonathan May},
  title     = {Using Syntax-Based Machine Translation to Parse English into Abstract
               Meaning Representation},
  journal   = {CoRR},
  volume    = {abs/1504.06665},
  year      = {2015},
  url       = {http://arxiv.org/abs/1504.06665},
  archivePrefix = {arXiv},
  eprint    = {1504.06665},
  timestamp = {Mon, 13 Aug 2018 16:48:21 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/PustHKMM15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

